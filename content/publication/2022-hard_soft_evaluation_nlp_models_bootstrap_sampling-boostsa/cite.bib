@inproceedings{fornaciari-etal-2022-hard,
    title = "Hard and Soft Evaluation of {NLP} models with {BOO}t{ST}rap {SA}mpling - {B}oo{S}t{S}a",
    author = "Fornaciari, Tommaso  and
      Uma, Alexandra  and
      Poesio, Massimo  and
      Hovy, Dirk",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-demo.12",
    doi = "10.18653/v1/2022.acl-demo.12",
    pages = "127--134",
    abstract = "Natural Language Processing (NLP) {`}s applied nature makes it necessary to select the most effective and robust models. Producing slightly higher performance is insufficient; we want to know whether this advantage will carry over to other data sets. Bootstrapped significance tests can indicate that ability.So while necessary, computing the significance of models{'} performance differences has many levels of complexity. It can be tedious, especially when the experimental design has many conditions to compare and several runs of experiments.We present BooStSa, a tool that makes it easy to compute significance levels with the BOOtSTrap SAmpling procedure to evaluate models that predict not only standard hard labels but soft-labels (i.e., probability distributions over different classes) as well.",
}
