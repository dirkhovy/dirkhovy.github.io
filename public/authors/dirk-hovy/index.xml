<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dirk Hovy</title>
    <link>https://dirkhovy.com/authors/dirk-hovy/</link>
      <atom:link href="https://dirkhovy.com/authors/dirk-hovy/index.xml" rel="self" type="application/rss+xml" />
    <description>Dirk Hovy</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©  Dirk Hovy, 2026</copyright><lastBuildDate>Tue, 25 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dirkhovy.com/images/icon_huf5895639c94b85425f6a15eb3aa47208_32923_512x512_fill_lanczos_center_3.png</url>
      <title>Dirk Hovy</title>
      <link>https://dirkhovy.com/authors/dirk-hovy/</link>
    </image>
    
    <item>
      <title>How to Write Gooder</title>
      <link>https://dirkhovy.com/post/2025_11_25/</link>
      <pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2025_11_25/</guid>
      <description>&lt;p&gt;After publishing &amp;ldquo;
&lt;a href=&#34;https://dirkhovy.com/post/2025_01_14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to professor&lt;/a&gt;&amp;rdquo;, several people said they found it helpful, and asked whether I had a similar post on writing. Luckily, we have held an annual writing workshop in the lab for the last few years, so there already was a presentation. This article is the text version of that presentation. As with the previous post, everything here is based on my own experience as an academic writer, with some grounding in prior work, so take it with a grain of salt. However, if it can help you improve your writing and avoid some of my mistakes, go ahead.&lt;/p&gt;
&lt;p&gt;Before we start, though, let&amp;rsquo;s address the elephant in the room. Some people have asked whether we still need to know how to write in the age of LLMs. My answer is an unambiguous &amp;ldquo;yes.&amp;rdquo; You should, of course, use all the tools at your disposal, e.g., 
&lt;a href=&#34;https://chatgpt.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt; to turn your notes into a continuous text, 
&lt;a href=&#34;https://quillbot.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quillbot&lt;/a&gt; to make your text more fluent, or 
&lt;a href=&#34;https://app.grammarly.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grammarly&lt;/a&gt; to improve the grammaticality. However, all of those tools are ML applications, and the old adage of &amp;ldquo;crap in, crap out&amp;rdquo; holds here as well. Starting out with a well-written piece will yield better results than leaving it to the tools to fix some sloppy copy. Besides, some LLM tools have their own flawed writing tendencies, such as overusing superlatives and empty phrases (&amp;ldquo;let&amp;rsquo;s delve into this fascinating topic&amp;rdquo;). They also tend not to be as good at structural aspects as human editors. Knowing how to write well will help you spot inconsistencies in LLM suggestions. More importantly, though, they do not reflect your voice, and that can leave the writing feel generic and anodyne. They do, however, make decent critics for early drafts to help you spot what is missing.&lt;/p&gt;
&lt;h2 id=&#34;scientific-writing&#34;&gt;Scientific Writing&lt;/h2&gt;
&lt;h3 id=&#34;the-importance-of-good-writing&#34;&gt;The Importance of Good Writing&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Good writing is like a window pane.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;mdash;&lt;em&gt;George Orwell&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Scientific writing is a fundamental skill for researchers and academics. Whether you are authoring a research paper or drafting a grant proposal, writing is crucial for clear and effective communication. No scientific paper is just the results: A surprising amount of academic writing is also storytelling. You need to frame the results and make the reader care about them (anecdotally, I have seen rejected papers get accepted after improving just the writing).&lt;/p&gt;
&lt;p&gt;Good writing differentiates successful communication from misunderstandings. Even small things, like misplaced words or punctuation errors, can dramatically alter meaning. For example, consider the headline: &amp;ldquo;Rachael Ray finds inspiration in cooking her family and her dog.&amp;rdquo; Poor dog&amp;hellip;
Adding prepositions like &amp;ldquo;with&amp;rdquo; and &amp;ldquo;for,&amp;rdquo; or simply using commas, can prevent such unfortunate interpretations. Clear writing helps avoid confusion.&lt;/p&gt;
&lt;p&gt;Many review forms specifically ask how readable a submission is. That might seem like a formality, but getting a low score here will likely obfuscate any strengths of your work in other areas.&lt;/p&gt;
&lt;h3 id=&#34;know-your-audience&#34;&gt;Know Your Audience&lt;/h3&gt;
&lt;p&gt;Scientific writing should be clear to any well-prepared reader. First impressions matter: You win or lose the reader on the first page, most likely already in the abstract. Within the first few lines, you either capture their attention and make them want to read on &amp;ndash; or you lose their interest (and potentially the paper acceptance). Engrossed reviewers will be much more inclined to rate your paper highly. They may even forgive minor details that are off later on. If you start out on the back foot instead, reviewers may feel you are wasting their time and will be much more critical of any minor detail.&lt;/p&gt;
&lt;p&gt;With that in mind, we should tailor our writing to fit our audience: a scientific paper and a blog post have different audiences, and your writing should take that into account. Independent of the topic, any audience falls somewhere on the &amp;ldquo;What-So What&amp;rdquo; curve, i.e., how much they care about the technical details (What) vs. the reasons and implications of the work (So What). Your co-authors will care much more about the What than the So What (they know why you did it). For most other audiences, the importance of the So What increases. The depth and complexity of information you&amp;rsquo;ll include differ greatly from what you&amp;rsquo;d present to a class of high-schoolers, a peer group, or grant reviewers (which might not be professional peers). Understanding your audience helps you decide how much background information to provide, how much jargon is acceptable, and how much you need to invest to make them care about the problem. People in your field are intrinsically motivated (otherwise, they would not be in that field). Grant reviewers might not. Depending on the type of grant, these might not be people in your field, or even academics at all.&lt;/p&gt;
&lt;p&gt;For any audience, though, make sure to reduce their cognitive load: Suspense is great in fiction, but in academic writing, spoilers are completely okay. Do not leave readers guessing what the paper is about; tell them upfront. Keeping track of 5 different scenarios that a research paper could evolve into is taxing even for experts. Save them the trouble and let them focus on the content.&lt;/p&gt;
&lt;p&gt;I have often heard the advice not to make your audience feel dumb. Most people, myself included, understand that to mean not to oversimplify matters. However, that is probably not correct. I have never seen an audience dislike a paper just becasue it explained a complex matter simply: Nobody feels dumb if they understand what&amp;rsquo;s going on in a paper. What &lt;em&gt;does&lt;/em&gt; make people feel dumb is if they don&amp;rsquo;t get what the paper is about. In a few cases, that might be due to their limited knowledge. In the overwhelming majority of cases, though, it&amp;rsquo;s due to bad writing.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;&amp;lsquo;All you have to do is write one true sentence. Write the truest sentence that you know.&amp;rsquo; So finally I would write one true sentence, and then go on from there. It was easy then because there was always one true sentence that I knew or had seen or had heard someone say.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;mdash;&lt;em&gt;Ernest Hemmingway&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We often think of writing as an artistic expression that requires some sort of innate talent. And while great fiction writers certainly bring their own style to the table, even they will readily admit that good writing is mainly one thing: practice. (Case in point: despite his adventurous lifestyle, Hemingway slavishly followed a daily writing routine and rewrote often.) The good news is that means nobody is automagically good at it. It also means anyone can learn it. Even if English is not your native language, you can craft good scientific texts by following some simple rules of thumb and practicing them until they become second nature. They might not win a Pulitzer Prize, but they will get your message across and keep your readers engaged. All it takes is some simple rules, repeated improvements, and lots of practice.&lt;/p&gt;
&lt;p&gt;I have been writing almost daily for about 25 years. I have published over 150 articles, two textbooks, and have a forthcoming trade book. I like writing. And yet, the first draft I write is still gonna be dogwater. I have accepted that and now try to get it out of the way as soon as possible. It&amp;rsquo;s better to have a mediocre first draft you can improve than wait forever for the perfect inspiration. And with a few basic guidelines in mind, you can make that first draft at least a good starting point.&lt;/p&gt;
&lt;h3 id=&#34;rules-of-thumb&#34;&gt;Rules of Thumb&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Short Sentences&lt;/strong&gt; are more effective and easier to understand. If a sentence extends beyond two lines, split it into smaller ones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subject Placement:&lt;/strong&gt; Introduce the subject early in the sentence to clarify who&amp;rsquo;s doing what, and put it close to the verb.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full Verbs:&lt;/strong&gt; Use full verbs rather than nominal constructions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Active Voice&lt;/strong&gt; helps with all of the above. Instead of saying, &amp;ldquo;Measurements were taken,&amp;rdquo; use &amp;ldquo;We measured.&amp;rdquo; Many of us were taught to use the passive voice in formal writing because it is considered more &amp;ldquo;objective.&amp;rdquo; In practice, it hides who did the work and also offloads responsibility. Be bold: you put all this effort into your work, stand for it!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Structure:&lt;/strong&gt; Introduce new concepts at the end of the sentence and elaborate on them immediately after. (It&amp;rsquo;s okay to break the active voice rule above to help with this aspect.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clear Referents:&lt;/strong&gt; avoid dangling pronouns/starting sentences with a pronoun: &amp;ldquo;This enables us&amp;rdquo; ⟶ &amp;ldquo;This &lt;em&gt;method&lt;/em&gt; enables us.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;avoiding-fluff&#34;&gt;Avoiding Fluff&lt;/h3&gt;
&lt;p&gt;Eliminate unnecessary words or phrases to make your text clearer. Here is an incomplete list for easy find-and-replace:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;at that point in time ⟶ then&lt;/li&gt;
&lt;li&gt;at this point in time ⟶ now&lt;/li&gt;
&lt;li&gt;in the event that ⟶ if&lt;/li&gt;
&lt;li&gt;until such time as ⟶ until&lt;/li&gt;
&lt;li&gt;on account of ⟶ because&lt;/li&gt;
&lt;li&gt;in the majority of cases ⟶ mostly&lt;/li&gt;
&lt;li&gt;has the capability of ⟶ can&lt;/li&gt;
&lt;li&gt;in spite of the fact that ⟶ despite&lt;/li&gt;
&lt;li&gt;in the final analysis ⟶ finally, ultimately&lt;/li&gt;
&lt;li&gt;a large percentage of ⟶ most&lt;/li&gt;
&lt;li&gt;owing to the fact that ⟶ because&lt;/li&gt;
&lt;li&gt;need to be established ⟶ requires&lt;/li&gt;
&lt;li&gt;give consideration to ⟶ consider&lt;/li&gt;
&lt;li&gt;with the exception of ⟶ except&lt;/li&gt;
&lt;li&gt;it would thus appear that ⟶ apparently&lt;/li&gt;
&lt;li&gt;in order to ⟶ to&lt;/li&gt;
&lt;li&gt;we aim to do X ⟶ we do X&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;illustrative-examples&#34;&gt;Illustrative Examples&lt;/h3&gt;
&lt;p&gt;Consider this before-and-after example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Before:&lt;/strong&gt; &amp;ldquo;By investigating the characteristics of bidirectional embeddings, it is desired that new future roles for this approach will be determined.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;After:&lt;/strong&gt; &amp;ldquo;Our investigation of bidirectional embeddings will reveal new roles for this approach.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The revised sentence is direct and easy to understand. It&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;has a subject-first sentence structure making clear who did what,&lt;/li&gt;
&lt;li&gt;uses full verbs,&lt;/li&gt;
&lt;li&gt;avoids the semantically empty passive construction,&lt;/li&gt;
&lt;li&gt;is shorter and more decisive in its language&lt;/li&gt;
&lt;li&gt;removes unnecessary fluff.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing-abstracts-and-introductions&#34;&gt;Writing Abstracts and Introductions&lt;/h3&gt;
&lt;p&gt;Given that you win over or lose the review early on, a compelling abstract is vital for your paper. It should entice reviewers to read on and engage with your work. A good abstract clearly outlines the area, purpose, results, and significance of your research, and tells the reader what to expect (remember: spoilers are good).&lt;/p&gt;
&lt;p&gt;Since abstracts are your calling card, an effective abstract should succinctly describe:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The general area&lt;/li&gt;
&lt;li&gt;The predominant current approach&lt;/li&gt;
&lt;li&gt;What is missing, or common problems with that approach (the &amp;ldquo;pain&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;What do you do instead (the &amp;ldquo;gain&amp;rdquo;)?&lt;/li&gt;
&lt;li&gt;How do you do it?&lt;/li&gt;
&lt;li&gt;Your results&lt;/li&gt;
&lt;li&gt;What that means for the future&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a practical algorithm, start by writing one sentence for each of those points. It will not be the most beautiful abstract, but it will get your point across. Personally, I am a fan of writing abstracts early, even if you have not yet started the work (advice I got from a PhD committee member). It helps you clarify your thoughts, gives you a marching plan, and lets you gauge interest in the topic by sharing it with colleagues. Any research will reach a point where you need to decide which direction to take it in. Having a written statement of what you thought you would do in the form of an abstract helps you decide which path to go down.&lt;/p&gt;
&lt;p&gt;That said, the abstract is the part of a paper that needs the most work, and that changes the most. It needs to accurately reflect the state of the work (which might have changed over time).&lt;/p&gt;
&lt;p&gt;As an example of how to overhaul an abstract, take the following one from an old review (slightly altered for anonymity):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The available statistical methods for sequential tagging, especially in maximal NP (noun phrase) chunking, perform well. However, existing tools cannot combine methods freely. All the freely available tools consist of tightly coupled modules. Combining well-known methods (HMM, Maximum Entropy model, TnT, first and second order transition model, beam search, CRFs, smoothing techniques) in a general-purpose sequential tagger would help to find the best combination for each task. This paper introduces an updated, modular, universal sequential tagger which was used for maximal NP chunking. The tool was rewritten from an existing project and its Maximum Entropy based HMM (MEMM) framework was completed with other common methods: TnT-like trigram transition model and the ability of drop-in replacement unigram model from any classifier from the Scikit-learn library or compatible API. The new tool is open source under the LGPL licence. Several kinds of method combinations were tested for English and Amharic. A trigram based solution resulted in the best ever F-score of 93.56% for Amharic NP chunking (+3% improvement). The key of the improvement is based on more accurate POS categories with fine-tuned trigram search.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It has all the information needed, but dives right in, jumps around between parts, and buries some of the information. Using the 7-sentence structure and some of the basic rules we have seen, we can rearrange and rewrite it like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Noun phrase (NP) chunking is a core method in linguistic analysis.&lt;/li&gt;
&lt;li&gt;The best NP chunking tools use statistical methods for sequential tagging.&lt;/li&gt;
&lt;li&gt;However, existing tools cannot combine methods freely to find the best combination for each task.&lt;/li&gt;
&lt;li&gt;We show how to combine well-known methods (different learners, models, and smoothing techniques) in a general-purpose sequential tagger.&lt;/li&gt;
&lt;li&gt;We introduce a modular, universal NP chunker, based on Maximum Entropy Markov Models, which includes other common methods, like drop-in.&lt;/li&gt;
&lt;li&gt;We test several combinations for English and Amharic and achieve the best reported F-score of 93.56% for Amharic NP chunking (+3% improvement).&lt;/li&gt;
&lt;li&gt;The improvement is based on more accurate POS categories with fine-tuned trigram search.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;A nice bonus of the 7-sentence abstract is that it also serves as a scaffold for your introduction section. Simply copy the abstract, expand each sentence into a paragraph by adding examples and references, and you have the outline of that section as well. Spend some time crafting a succinct example that encapsulates what you are working on. Make clear why it matters. Bonus points if you can reuse the example throughout the paper.&lt;/p&gt;
&lt;h2 id=&#34;grant-writing-essentials&#34;&gt;Grant Writing Essentials&lt;/h2&gt;
&lt;p&gt;At some point, you will find yourself in a position where you have to write a grant. Maybe it&amp;rsquo;s your choice, but it might also be a requirement to get tenure or a promotion. Either way, grantwriting has a couple of benefits independent of whether you end up getting the money.
It sharpens your research ideas, forces you to think long-term, and helps you practice outreach to non-academics (many grant reviewers are not academics).&lt;/p&gt;
&lt;p&gt;The mistake I made when writing my first grant (which, from what I&amp;rsquo;ve heard over the years, is common) was treating it as just a longer research paper. However, they are patently not. Writing grant proposals differs from writing academic papers in several ways. The What–So-What tradeoff is very different from a paper. It&amp;rsquo;s probably better to think of grants as more like blog posts: they should be understandable to a broad, self-selected audience, make a succinct point, and be somewhat exciting.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Papers&lt;/th&gt;
&lt;th&gt;Grants&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Audience&lt;/td&gt;
&lt;td&gt;Researcher with scholarly passion&lt;/td&gt;
&lt;td&gt;Sponsor-centered: Service attitude&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Focus&lt;/td&gt;
&lt;td&gt;Past work you have done&lt;/td&gt;
&lt;td&gt;Future work you wish to do&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Function&lt;/td&gt;
&lt;td&gt;Exposition: Explaining to reader&lt;/td&gt;
&lt;td&gt;Persuasive: “Sell” the reader&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Style&lt;/td&gt;
&lt;td&gt;Impersonal, objective, dispassionate&lt;/td&gt;
&lt;td&gt;Personal: Convey excitement&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Organization&lt;/td&gt;
&lt;td&gt;Individualistic: present your idea&lt;/td&gt;
&lt;td&gt;Team-oriented: Feedback needed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Specificity&lt;/td&gt;
&lt;td&gt;Specialized terminology ok&lt;/td&gt;
&lt;td&gt;Accessible general language&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Themes&lt;/td&gt;
&lt;td&gt;Theses, theories, and ideas&lt;/td&gt;
&lt;td&gt;Goals, activities, and outcomes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;grant-questions&#34;&gt;Grant Questions&lt;/h3&gt;
&lt;p&gt;Pick a problem you believe in and ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why is it important?&lt;/li&gt;
&lt;li&gt;How is existing knowledge or practice inadequate?&lt;/li&gt;
&lt;li&gt;Why is your idea better?&lt;/li&gt;
&lt;li&gt;How is it new, unique, different?&lt;/li&gt;
&lt;li&gt;What will it contribute and who will benefit from it?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also identify whether it&amp;rsquo;s an individual or consortium grant, and whether it&amp;rsquo;s top-down (they provide the topics) or bottom-up (you come up with an idea). Those aspects will matter in whether you start writing right away or find some collaborators first, and how you frame the idea, respectively.&lt;/p&gt;
&lt;h3 id=&#34;pitch-perfect&#34;&gt;Pitch Perfect&lt;/h3&gt;
&lt;p&gt;Make a compelling case for your idea&amp;rsquo;s impact and benefits. Ideally, the grant topic is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an important need or issue that should be addressed&lt;/li&gt;
&lt;li&gt;a gap between where we are now and where we could be&lt;/li&gt;
&lt;li&gt;a limitation of current knowledge or way of doing things holding us back&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All this is similar to the &amp;ldquo;pain&amp;rdquo; we have seen in Abstract writing. I have heard this called the &amp;ldquo;unrejectionable &amp;ldquo;: what major issue do the funders allow to continue by not funding your proposal?&lt;/p&gt;
&lt;p&gt;In contrast, your grant project is an opportunity to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;advance our understanding or address a societal need&lt;/li&gt;
&lt;li&gt;improve efficiency or lower costs of goods and/or services&lt;/li&gt;
&lt;li&gt;reshape our thinking or way of doing things&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And yes, that is the &amp;ldquo;gain&amp;rdquo; we have seen earlier.&lt;/p&gt;
&lt;p&gt;With all that in mind, set the stage to engage and excite the reviewers. Remember: You win or lose on the first page!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify the need/importance: Who cares?&lt;/li&gt;
&lt;li&gt;Describe your project&amp;rsquo;s fundamental purpose&lt;/li&gt;
&lt;li&gt;Create a vision (&amp;ldquo;So What?&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;Summarize the state of the art&lt;/li&gt;
&lt;li&gt;Show how your work will advance the field&lt;/li&gt;
&lt;li&gt;State your solution clearly&lt;/li&gt;
&lt;li&gt;Describe technical challenges to solving the problem&lt;/li&gt;
&lt;li&gt;Describe the concept and establish credibility&lt;/li&gt;
&lt;li&gt;Outline potential benefits&lt;/li&gt;
&lt;li&gt;Envision the world with the problem solved&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;keywords-and-key-words&#34;&gt;Keywords and Key Words&lt;/h3&gt;
&lt;p&gt;Clearly state the project&amp;rsquo;s goal and how it aligns with the grant&amp;rsquo;s objectives. They will give you specific keywords, sometimes explicitly: Pay attention to them and what they mean. Make sure you use those words (and maybe even boldface them) in your proposal.&lt;/p&gt;
&lt;p&gt;Some other &amp;ldquo;grant&amp;rdquo; words:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Objectives&lt;/em&gt;: The big goals to solve. State them explicitly and refer back to them.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Work packages&lt;/em&gt;: Steps in your pipeline (e.g., data collection, model development, literature review).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Milestones&lt;/em&gt;: Achievements. Release of a model, data set, etc. These are often the results of work packages.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Deliverables&lt;/em&gt;: Overall outcomes of the project: e.g., a website, publications, models, data sets, workshops, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Stakeholders&lt;/em&gt;: Society, academia, industry, and grant funders. Ideally, they all benefit from your findings in some way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;money-money-money&#34;&gt;Money, Money, Money&lt;/h3&gt;
&lt;p&gt;Your grant officer should help you with the details, but you need to have the basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;personnel (postdocs, PhDs, RAs, annotators)&lt;/li&gt;
&lt;li&gt;teaching buyout/time costs&lt;/li&gt;
&lt;li&gt;travel costs (flight, hotel, registration)&lt;/li&gt;
&lt;li&gt;publication costs&lt;/li&gt;
&lt;li&gt;equipment (GPU servers, eye trackers, etc.)&lt;/li&gt;
&lt;li&gt;data or annotation services (student annotators, MTurk, surveys, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Account for all expenses, and make them make sense in the narrative. I.e., do not ask for something that is not justified by the project.&lt;/p&gt;
&lt;p&gt;Universities will also take a fixed percentage of &lt;em&gt;Overhead Costs&lt;/em&gt;: think of this as a fee for using their facilities and support. It varies by university, from 30% to over 60%. Some grants explicitly include it, others do not. Be sure to check!&lt;/p&gt;
&lt;h3 id=&#34;common-mistakes&#34;&gt;Common Mistakes&lt;/h3&gt;
&lt;p&gt;Frequent (preventable) reasons for rejection include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposal did not match program/keywords&lt;/li&gt;
&lt;li&gt;Applicant did not follow directions&lt;/li&gt;
&lt;li&gt;Problem statement is too global&lt;/li&gt;
&lt;li&gt;Problem has no relationship to reality, or no potential solution&lt;/li&gt;
&lt;li&gt;Problem has been studied to death already&lt;/li&gt;
&lt;li&gt;Unclear problem, central theme, methods, or goals&lt;/li&gt;
&lt;li&gt;Overly dramatic imagined consequences&lt;/li&gt;
&lt;li&gt;Jargon and lack of clear writing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond that, it&amp;rsquo;s of course up to the reviewers and funders whether they like your grant, but it&amp;rsquo;s better to avoid getting sorted out at an early stage already.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Both scientific papers and grant proposals should be clearly written to effectively communicate their potential impact and win over reviewers. The tools and strategies outlined here give you a scientific writing foundation to achieve that.
Beyond that, aim for clarity, structure, and persuasiveness. Use concise language and tailor your message to your audience for the best results. But most of all: start early, keep iterating on the draft, get lots of feedback, and have fun with it. Happy writing!&lt;/p&gt;
&lt;h3 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h3&gt;
&lt;p&gt;When I wrote my thesis, I discovered &lt;strong&gt;A Manual for Writers of Research Papers, Theses, and Dissertations: Chicago Style for Students and Researchers&lt;/strong&gt; by Kate Turabian. I wish I had known about it earlier. It helped me a lot. Some parts can be safely ignored if you use modern typesetting and word processors (e.g., how to make tables or insert images), but the stylistic advice still stands.
Beyond that, I find any well-crafted written piece, like articles in &lt;em&gt;The New Yorker&lt;/em&gt;, helpful to see how to draft better. Find a writer you like and see what they do to achieve their goals. It will help you get more comfortable and make your writing more compelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Consistency is Key: Disentangling Label Variation in Natural Language Processing with Intra-Annotator Agreement</title>
      <link>https://dirkhovy.com/publication/2025-consistency-key-disentangling-label-variation-nlp-intra-annotator-agreement/</link>
      <pubDate>Sun, 09 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-consistency-key-disentangling-label-variation-nlp-intra-annotator-agreement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models</title>
      <link>https://dirkhovy.com/publication/2025-no-for-some-yes-for-others-persona-prompts-and-other-sources-of-false-refusal-in-language-models/</link>
      <pubDate>Sun, 09 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-no-for-some-yes-for-others-persona-prompts-and-other-sources-of-false-refusal-in-language-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance</title>
      <link>https://dirkhovy.com/publication/2025-principled_personas/</link>
      <pubDate>Sun, 09 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-principled_personas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification?</title>
      <link>https://dirkhovy.com/publication/2025-co-detect/</link>
      <pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-co-detect/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biased Tales: Cultural and Topic Bias in Generating Children’s Stories?</title>
      <link>https://dirkhovy.com/publication/2025-biased-tales/</link>
      <pubDate>Tue, 09 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-biased-tales/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance</title>
      <link>https://dirkhovy.com/publication/2025-issuebench/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-issuebench/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The AI Gap: How Socioeconomic Status Affects Language Technology Interactions</title>
      <link>https://dirkhovy.com/publication/2025-ses-survey/</link>
      <pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2025-ses-survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals&#39; Subjective Text Perceptions</title>
      <link>https://dirkhovy.com/publication/2025-beyond-demographics-fine-tuning-large-language-models-predict-individuals-subjective-text-perceptions/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0100</pubDate>
      <guid>https://dirkhovy.com/publication/2025-beyond-demographics-fine-tuning-large-language-models-predict-individuals-subjective-text-perceptions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Educators&#39; Perceptions of Large Language Models as Tutors: Comparing Human and AI Tutors in a Blind Text-only Setting</title>
      <link>https://dirkhovy.com/publication/2025-educators-perceptions-large-language-models-tutors-comparing-human-ai-tutors-blind-text-only-setting/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0100</pubDate>
      <guid>https://dirkhovy.com/publication/2025-educators-perceptions-large-language-models-tutors-comparing-human-ai-tutors-blind-text-only-setting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Socially Aware Language Technologies: Perspectives and Practices</title>
      <link>https://dirkhovy.com/publication/2025-socially-aware-language-technologies-perspectives-practices/</link>
      <pubDate>Sun, 01 Jun 2025 00:00:00 +0100</pubDate>
      <guid>https://dirkhovy.com/publication/2025-socially-aware-language-technologies-perspectives-practices/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to professor</title>
      <link>https://dirkhovy.com/post/2025_01_14/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2025_01_14/</guid>
      <description>&lt;p&gt;It feels not long ago that I was worried about my job applications, but in reality, I have done this long enough that some of my first MSc supervisees are professors of their own now. Every few months, a lab member goes on the academic job market, and we have a conversation about how to best approach it.&lt;/p&gt;
&lt;p&gt;So I decided to collect those thoughts on what to do if you plan to become a professor (or, more generally, a professional researcher) in NLP. It&amp;rsquo;s entirely based on my own experience: I&amp;rsquo;ve worked two tenure track jobs, one more research-focused in a CS dept and one with more teaching in a Business School. I fact-checked with some people, but it&amp;rsquo;s probably still not always generalizable.&lt;/p&gt;
&lt;p&gt;If it helps you make up your mind, though, go ahead :)&lt;/p&gt;
&lt;h2 id=&#34;where-to-go&#34;&gt;Where to go&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The first question is where to go. Part of that is your active decision: Do you enjoy &lt;strong&gt;teaching or research&lt;/strong&gt; more? Part of that is out of your hands: There might not be a position in the city, region, or even country you would like to go to. So you will have to be flexible, but it pays to have a &lt;strong&gt;robust set of criteria&lt;/strong&gt;. Also, keep an open mind. There are plenty of options: &lt;strong&gt;traditional university jobs, business schools, think tanks, industry research labs, or government research facilities&lt;/strong&gt;. There might be even more. They all come with pros and cons, but it is a good idea to include all applicable ones in your search. Much of what follows is outside your control, but choosing to apply (and where) is most definitely within it.&lt;/p&gt;
&lt;h2 id=&#34;finding-a-position&#34;&gt;Finding a position&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Give it your best go, but do not get discouraged&lt;/strong&gt;. You do not know what a committee is looking for. They might look for people with a specific research profile, a specific demographic, or a specific skill. While this uncertainty is grating, it can also be consoling. You might not have gotten rejected because you were not good enough in the things you have control over – you might just not be the person they want. Nothing you can do about that. After being placed in the first position, I once got rejected because the hiring department decided to build a bridge to another department that required a specific subject area. One of the other candidates happened to have that specialization. I did not. They hired him. It wasn&amp;rsquo;t part of the initial call, and it wasn&amp;rsquo;t something they had planned. It happens.&lt;/p&gt;
&lt;p&gt;Some (though luckily not all) positions you apply for are already spoken for but have to run a public call nonetheless. You will do the whole song and dance, but you never actually had a chance to get it. Unfortunately, being rejected from a competition that was never open holds very little information. You will not know beforehand which kind of competition it is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expect to apply for 20 positions or more&lt;/strong&gt; for any match (depending on the field). Try to &lt;strong&gt;get practice interviews&lt;/strong&gt;. Your first interview should not be for your dream job.&lt;/p&gt;
&lt;p&gt;In general, you will be judged by these four aspects, in descending order of importance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Research productivity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Funding ability&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Teaching&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Service&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;1-research-productivity&#34;&gt;1. Research productivity&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Different places have very different ways of measuring research productivity, from your Google Scholar index, to a list of preferred venues, to a list of your three most impactful papers. It is impossible to optimize for all. You will not know beforehand which venues a university values (they might not even know themselves), so it&amp;rsquo;s good to go for a mixed strategy. The easiest metric to track is your h-index. It is a measure of both the quality and quantity of your research. &lt;strong&gt;Aim for the top venues in the field&lt;/strong&gt;, but throw in some &amp;ldquo;easy&amp;rdquo; venues with higher acceptance rates to &lt;strong&gt;bulk out the CV&lt;/strong&gt;. Often, those supposedly second-tier venue publications garner more citations than top venues. Having a lot of papers and a lot of citations is a helpful signal to employers. However, even candidates with high citation count who have only second-tier publications will likely get sorted out early if there is any expert in the committee.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do not be shy to self-cite&lt;/strong&gt;! I have heard the argument in a tenure case that &amp;ldquo;If even he doesn&amp;rsquo;t cite himself, how important is the work?&amp;rdquo; Ideally, you are developing a stream of research, so put in citations to your own prior work (if appropriate, of course). Before submitting a camera-ready, go over your prior work and check: would it make sense to include it here? It&amp;rsquo;s easy to overlook a paper that could add a valid point, even if it&amp;rsquo;s your own. Being able to cite your own prior work also indicates that you have a &lt;strong&gt;consistent research profile&lt;/strong&gt;. Also, seeing a reference in a paper makes others more likely to re-cite it.&lt;/p&gt;
&lt;p&gt;Research is a crowded business, and it is hard to keep track. People might not cite you for any number of reasons, but typically, it is simply a lack of awareness. &lt;strong&gt;Be active on social media&lt;/strong&gt; and &lt;strong&gt;make it easy for people to find you. Maintain an updated website and tell people what you are researching (and why).&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-funding-ability&#34;&gt;2. Funding ability&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Universities care about your research. They also care about money. If you have a &lt;strong&gt;proven track record of getting grants&lt;/strong&gt;, you will be much more desirable as a candidate than someone with the same CV but no grants. Grants also signal that your research is &lt;strong&gt;interesting to outside stakeholders&lt;/strong&gt;, something universities value. People typically only list their successful grants, the ones they got but declined, and maybe the current ones.&lt;/p&gt;
&lt;p&gt;Grant writing is very different from academic writing: it is story-telling, not research. Do not expect to be good at it from the start. It takes practice, and the odds are low, but persist, and you will be rewarded. As always, grant systems are &lt;strong&gt;auto-correlated&lt;/strong&gt;. If you got a grant in the past, you are more likely to get one in the future (they even ask you to list past grants to assess you). So &lt;strong&gt;aim for small grants first&lt;/strong&gt;, even if they are primarily symbolic. They will pad your resume and increase your chance for future grants.&lt;/p&gt;
&lt;p&gt;Do write grants. Lots of them. You will not be naturally good at it; it is a weird genre. Accept that most will get rejected and that you might not know why. If you get a success rate of 30% over your lifetime, you will be outstanding. Do go back and read old grants, and think about whether you would have given yourself money. Read other people&amp;rsquo;s grants and learn to parse them into components.&lt;/p&gt;
&lt;p&gt;Realize that you are &lt;strong&gt;writing for a lay audience&lt;/strong&gt;. The &amp;ldquo;so what&amp;rdquo; of the grant is equally or more important than the &amp;ldquo;what.&amp;rdquo; Identify the need for your work, and explain why we will be better off afterward (the &amp;ldquo;unrejectionable&amp;rdquo;): It is the funders&amp;rsquo; choice whether they want to let this problem persist…&lt;/p&gt;
&lt;p&gt;Grants give you &lt;strong&gt;freedom and independence&lt;/strong&gt;. Freedom to explore the areas you want and independence from university politics. Grants will often allow you to &lt;strong&gt;reduce your teaching load&lt;/strong&gt; to have more time for research. They will also give you a bargaining chip in &lt;strong&gt;salary negotiations&lt;/strong&gt; and other decisions.&lt;/p&gt;
&lt;h3 id=&#34;3-teaching&#34;&gt;3. Teaching&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;If you apply for an academic position, you will be expected to teach. There are sometimes incentives for this (teaching awards as positive, student evaluations as a negative incentive), but it will be a given. You should have a &lt;strong&gt;teaching statement&lt;/strong&gt; and some prepared material, but do not sweat this too much: early on, you are not expected to have a ton of teaching already. For better or worse, &lt;strong&gt;teaching quality rarely enters into hiring decisions&lt;/strong&gt; (we all had that one prof).
Fight hard to &lt;strong&gt;get a class on a topic you know&lt;/strong&gt; well, and fight even harder to keep it. Preparing a new class is a bottomless time sink, especially in areas you do not know well. If you can use the materials of a more experienced colleague, do so. Resist the urge to make your own or to tinker too much. If you have to make your own class from scratch, make sure you will teach it for the foreseeable future. Plan ahead. Make it count. You do not have to have the perfect material in your first year. &lt;strong&gt;It takes about three times to get a class fine-tuned&lt;/strong&gt; and make it your own. Don&amp;rsquo;t expect to know everything after one year.
More than with anything else, &lt;strong&gt;perfection is the enemy of the good&lt;/strong&gt; in teaching prep. You can spend any amount of time preparing. &lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t. &lt;/p&gt;
&lt;p&gt;Realize that &lt;strong&gt;students are not out to expose you&lt;/strong&gt;: They want to learn from you. You do not have to know every obscure detail. If you do not know some obscure detail, say so, make a note, and look it up for next time. It shows you are engaged, and it is an excellent chance for you to learn.&lt;/p&gt;
&lt;p&gt;Prepare an outline of the class, and think about what you want students to learn from it. &lt;strong&gt;Gear everything towards that learning outcome&lt;/strong&gt;. Refer back to that outline often. Do not be afraid to change it if you see it makes more sense in a different order. You will not know ahead of time. Use plenty of examples, and wrap up each class with take-home points. Worry more about the learning outcome than about grade curves or evaluations. Teaching can be gratifying and rewarding if you learn what works for you. It is worth investing in that. It can be a great complement to research, or it can be a drag. It is up to you how to approach it.&lt;/p&gt;
&lt;p&gt;Know that &lt;strong&gt;evaluations are not objective&lt;/strong&gt;. They measure how much students liked you as much as how well they learned. &lt;strong&gt;If you are female, more junior, or not white, your evaluations will likely be worse than that of white, male, more senior colleagues&lt;/strong&gt;. Use this to discount the numerical outcomes. &lt;/p&gt;
&lt;p&gt;Do &lt;strong&gt;pay attention to valid concerns&lt;/strong&gt; in the written part, but be prepared for contradictions. I have been told in one of my first classes that I was simultaneously too fast, too slow, too technical, not technical enough, too strict, and the nicest teacher all year. &lt;strong&gt;Discard the emotions&lt;/strong&gt; and look for constructive feedback.&lt;/p&gt;
&lt;p&gt;Structure your interaction with students and be transparent but firm about it. It is usual for them to have many questions, and it is easy to send an email. But not every question needs an email. There are more of them than you, and you have limited time. Even if every student just has a single question, that still means dozens of emails for you. Collect email questions and answer them in batch in class. Many will also resolve themselves (we all ask questions and realize the answer once we say it aloud). Point to the syllabus, write everything out as clearly as possible and refuse to answer things that are on there (and tell them you do that).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Only meet during dedicated office hours and after appointments&lt;/strong&gt; (I tried an open-door policy and did not get anything done). It&amp;rsquo;s also good to set a &lt;strong&gt;time limit for meetings&lt;/strong&gt;: if you have an online system, use it. Otherwise, set up a Google Form with name, time, and topic, and link to it on your website. Ask students to come prepared to those meetings. It will help them focus their thoughts, and it will make it easier to answer them.&lt;/p&gt;
&lt;h3 id=&#34;4-service&#34;&gt;4. Service&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Research is a community effort. Show that you are a &lt;strong&gt;good citizen&lt;/strong&gt;. If a university wants to hire you potentially for life, they like to see that you are involved. Get engaged in organizing workshops, panels, or initiatives (either write a proposal, or ask an existing committee to let you join). Choose the ones that align with your profile. Become a regular reviewer for a good journal, or eventually an editor.&lt;/p&gt;
&lt;p&gt;Take some time to &lt;strong&gt;understand how your workplace functions&lt;/strong&gt;: structurally, procedurally, politically. Those things are not the most natural fit for researchers and can be quite dry and boring, but knowing and understanding them can make your life easier and your work more effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure&lt;/strong&gt; is the number and types of department, research centers, labs, hubs, groups, etc. It’s the positions and roles that make up the governance of a place. Knowing who’s responsible for what can help you waste less time by asking the right person.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Procedure&lt;/strong&gt; is everything from grant management to hiring. What are the steps, which forms do you have to complete, who needs to sign off? This will make your work run more smoothly and save you unpleasant surprises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Politics&lt;/strong&gt; is everything that connects the former two, but on a personal level. Who works with whom, who wants what position, what are their goals and motivations. If a professor is known to hate AI, don’t waste time trying to convince them of your new hire or grant proposal. If two people don’t speak to each other, be careful not to get in the middle.&lt;/p&gt;
&lt;p&gt;Agree to help out with &lt;strong&gt;one committee in your university&lt;/strong&gt;, but choose which one. Say no to all others. Especially junior female professors are often expected to do a lot of service, and it can easily take over too much time that should go to research and building your profile.&lt;/p&gt;
&lt;h2 id=&#34;publicity&#34;&gt;Publicity&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Ideally, good work would bubble to the top. That&amp;rsquo;s not what happens, though. All things being equal, research from big schools or well-known co-authors get cited more. Research that was promoted on social media and is easy to find gets cited more. Research that made it into the news gets cited more. As a researcher, it is your job to publish and then publicize your findings. Getting it into a venue is &lt;strong&gt;not&lt;/strong&gt; enough. &lt;/p&gt;
&lt;p&gt;You need a website that is easy to find and up to date. You need to post things on social media: talks you give, workshops you organize, papers you published. &lt;strong&gt;Make it easy for people to find your work and understand it&lt;/strong&gt;. If two people talk about your area, they need to mention your name. It is not their job to be aware of you. It is your job to make them aware. &lt;strong&gt;You will have to say the same thing many times before people know it.&lt;/strong&gt; Have a clear message and profile, and reinforce it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network widely&lt;/strong&gt;, go to conferences, give invited talks. Give plenty of invited talks. &amp;ldquo;Invited&amp;rdquo; does not mean they cold-called you. If you visit a city with a relevant research group, let them know and offer to give a talk. They will usually be happy to slot you in.&lt;/p&gt;
&lt;p&gt;When you go up for tenure, schools will send out requests for reference letters from people you haven&amp;rsquo;t worked with. Cultivate a network of those people early! You will be able to list some reference givers, but they might choose some more. So make sure your name is known.&lt;/p&gt;
&lt;p&gt;Making it into the press is usually a matter of supplying your press office with a release and hoping they get it out or promoting it on Twitter. &lt;strong&gt;Newspapers rarely trawl conference proceedings&lt;/strong&gt;. Journalists are similar to researchers in that they are looking for interesting new findings. However, they are under much more time and space constraints, which often does not leave room for subtlety. When giving an interview, prepare: &lt;strong&gt;What single sentence do you want them to take away from the interview&lt;/strong&gt;? Repeat that one. Make your points clear and simple: people have no time to read up on the background knowledge. Use simple examples and images. Take notes of all the things you want to cover in the interview and make sure to tick them off as you go. &lt;strong&gt;Ask for written questions ahead of time&lt;/strong&gt;. After the interview, &lt;strong&gt;ask for a draft&lt;/strong&gt; or at least the quotes of you before publication. Sharing this should be standard, but sometimes it is not. Those proofs are your only chance to correct the record if you have been misunderstood (it happens, even with preparation). Remember that &lt;strong&gt;you can mark certain things as &amp;ldquo;off the record.&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;staying-creative-and-managing-projects&#34;&gt;Staying creative and managing projects&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Whatever route you take, you will have to generate new research ideas. When you are in industry, you should never be irreplaceable. When you are in academia, you need to be one of a kind. Either way, you should be original and creative. It does get easier over time, but it takes work. &lt;strong&gt;Find out what inspires you&lt;/strong&gt;, and seek that out. For me, it is interacting with people on visits or conferences and then distracting myself with something creative but unrelated to push it forward. Trust your instinct: if you need a day of slacking and playing video games, it might just be your brain recharging and making new connections.&lt;/p&gt;
&lt;p&gt;You will have to &lt;strong&gt;find out how many parallel projects you can pursue&lt;/strong&gt; to always have an iron in the fire without slowing progress on everything. It should be more than one, but the exact number depends on your multi-tasking affinity and the projects&amp;rsquo; complexity. For some, that is 2. For others, 15. &lt;strong&gt;Try to push yourself to find that limit, but realize when you reach it&lt;/strong&gt;. If you feel like you are standing still, don&amp;rsquo;t be afraid to shelf a project.&lt;/p&gt;
&lt;p&gt;Those projects ideally fit together to advance your research profile, but do not be afraid to try something new out every once in a while. Nothing is sadder than a researcher clinging to outdated ideas.&lt;/p&gt;
&lt;p&gt;How much you can choose and experiment also depends on your career stage. Earlier on, it is probably reasonable to explore more and to push many lines. As you find your niche, focus on that. At the same time, getting &lt;strong&gt;tenure allows you to be more experimental&lt;/strong&gt;. Use the security to explore more far-fetched and ambitious projects, but don&amp;rsquo;t risk your collaborators&amp;rsquo; careers.&lt;/p&gt;
&lt;p&gt;But do not get stuck in minor variations of the same theme either.&lt;/p&gt;
&lt;h2 id=&#34;running-a-lab&#34;&gt;Running a lab&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Depending on your choices, you might have the chance to build up a lab. This is sometimes even expected. If you have a choice, consider the options. &lt;strong&gt;Working with a lab allows you to explore more research areas&lt;/strong&gt;, but it will replace a large chunk of your time with &lt;strong&gt;managerial duties, admin, and bureaucracy&lt;/strong&gt;. That chunk usually comes out of your active research time. Working alone means less collaboration on a day-to-day basis, but also no managerial responsibilities. Either way, fight hard to &lt;strong&gt;keep a day clear of all meetings&lt;/strong&gt; and admin to do your own research. Mark it in your calendar as blocked, and &lt;strong&gt;do not make exceptions&lt;/strong&gt;. Ever. If you make an exception for a good reason, you&amp;rsquo;ll soon make one for a bad reason.&lt;/p&gt;
&lt;p&gt;If you do build a lab, realize that you will spend more time each day with these people than with your partner. &lt;strong&gt;Don&amp;rsquo;t hire assholes&lt;/strong&gt;. Work on the group atmosphere you want: you will set the tone and the norms. Treat people with respect and trust them that they want to do good work. &lt;strong&gt;Do not be afraid to give up responsibility and control&lt;/strong&gt;. &lt;strong&gt;Do not make them do something you would not do.&lt;/strong&gt; Be clear about what you expect from them, and keep their desks free of unnecessary bureaucracy and admin. Listen to their ideas and concerns, and take them seriously. Say &amp;ldquo;Yes, and?&amp;rdquo; and see where it takes them. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is your job to help them succeed&lt;/strong&gt; and solve their problems, not the other way around.&lt;/p&gt;
&lt;p&gt;Be flexible and &lt;strong&gt;don’t be afraid to change things&lt;/strong&gt;. The meeting day does not work for most of the group? Find a different day. The format of the internal talks feels boring? Try shortening it, restrict the topics, make it an interpretative dance. You are not forced to stick to something if it does not work. Occasionally, it can be good to try something new even if nobody feels the need. You can always change it back.&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Make sure to take at least one day a week completely off. That also means no email. Keep some hobbies, especially sports. You can get away with a lot of neglect in your 20s, but it will catch up, and you don’t want to lose time fixing a preventable injury. Our jobs require a lot of sitting at a computer: counteract that with whatever sports you enjoy.&lt;/p&gt;
&lt;p&gt;Go on vacation, and set your OOO response for one more day than you are away. You can use it to catch up on email.&lt;/p&gt;
&lt;p&gt;Don’t feel like you have to be perfect. Make a plan, be prepared, but know that things will blindside you, that things will not work out. And that’s ok. So will you be. I have failed many times, and usually it didn’t matter. Our jobs are not heart surgery.&lt;/p&gt;
&lt;p&gt;Lastly, realize that you are given one of the potentially best and most self-directed jobs there are, but that it’s still a job. Don’t let it distract you from the importance of friends, family, and your health.&lt;/p&gt;
&lt;p&gt;(Some of the better advice in here is based on tips from Dan Jurafsky, Ed Hovy, Lyle Ungar, and Nel Dutt. Thanks!)&lt;/p&gt;
&lt;p&gt;“Things of great importance have to be taken lightly.” &lt;/p&gt;
&lt;p&gt;– &lt;em&gt;Hagakure&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;“Realize that your choices are half chance. So are everybody else&amp;rsquo;s.” &lt;/p&gt;
&lt;p&gt;– &lt;em&gt;Baz Luhrman, &amp;ldquo;Everybody&amp;rsquo;s free (to wear sunscreen)&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models</title>
      <link>https://dirkhovy.com/publication/2024-divine-llamas-emotion-bias/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-divine-llamas-emotion-bias/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models</title>
      <link>https://dirkhovy.com/publication/2024-compromesso/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-compromesso/</guid>
      <description></description>
    </item>
    
    <item>
      <title>My Answer is C: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models</title>
      <link>https://dirkhovy.com/publication/2024-myanswerisc/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-myanswerisc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models</title>
      <link>https://dirkhovy.com/publication/2024-politicalcompass/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-politicalcompass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</title>
      <link>https://dirkhovy.com/publication/2024-xstest/</link>
      <pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-xstest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DADIT: A Dataset for Demographic Classification of Italian Twitter Users and a Comparison of Prediction Methods</title>
      <link>https://dirkhovy.com/publication/2024-dadit/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-dadit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts</title>
      <link>https://dirkhovy.com/publication/2024-difficulty-classification/</link>
      <pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-difficulty-classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety </title>
      <link>https://dirkhovy.com/publication/2024-safetyprompts/</link>
      <pubDate>Mon, 08 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-safetyprompts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution</title>
      <link>https://dirkhovy.com/publication/2024-emotion-gender-stereotypes/</link>
      <pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-emotion-gender-stereotypes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conversations as a Source for Teaching Scientific Concepts at Different Education Levels</title>
      <link>https://dirkhovy.com/publication/2024-conversations-data/</link>
      <pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-conversations-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions</title>
      <link>https://dirkhovy.com/publication/2024-emotion-analysis-survey/</link>
      <pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-emotion-analysis-survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Classist Tools: Social Class Correlates with Performance in NLP</title>
      <link>https://dirkhovy.com/publication/2024-socialclass-experiments/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-socialclass-experiments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impoverished Language Technology: The Lack of (Social) Class in NLP</title>
      <link>https://dirkhovy.com/publication/2024-socialclass-survey/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2024-socialclass-survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation</title>
      <link>https://dirkhovy.com/publication/2023-label-variation-llms/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-label-variation-llms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MilaNLP at SemEval-2023 Task 10: Ensembling Domain-Adapted and Regularized Pretrained Language Models for Robust Sexism Detection</title>
      <link>https://dirkhovy.com/publication/2023-milanlp-semeval-2023-task-10-ensembling-domain-adapted-regularized-pretrained-language-models-robust-sexism-detection/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-milanlp-semeval-2023-task-10-ensembling-domain-adapted-regularized-pretrained-language-models-robust-sexism-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech</title>
      <link>https://dirkhovy.com/publication/2023-zero-shot-prompting-hate-speech/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-zero-shot-prompting-hate-speech/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Temporal and Second Language Influence on Intra-Annotator Agreement and Stability in Hate Speech Labelling</title>
      <link>https://dirkhovy.com/publication/2023-temporal-second-language-influence-intra-annotator-agreement-stability-hate-speech-labelling/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-temporal-second-language-influence-intra-annotator-agreement-stability-hate-speech-labelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics</title>
      <link>https://dirkhovy.com/publication/2023-ecological-fallacy-annotation-modeling-human-label-variation-goes-beyond-sociodemographics/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-ecological-fallacy-annotation-modeling-human-label-variation-goes-beyond-sociodemographics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The State of Profanity Obfuscation in Natural Language Processing Scientific Publications</title>
      <link>https://dirkhovy.com/publication/2023-prof-profanity-obfuscation-nlp/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-prof-profanity-obfuscation-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What about &#39;&#39;em&#39;&#39;? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns</title>
      <link>https://dirkhovy.com/publication/2023-commercial-machine-translation-fail-neopronouns/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-commercial-machine-translation-fail-neopronouns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What about &#39;&#39;em&#39;&#39;? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns</title>
      <link>https://dirkhovy.com/publication/2023-interpretability-for-fairer-machine-translation/</link>
      <pubDate>Wed, 12 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-interpretability-for-fairer-machine-translation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging Social Interactions to Detect Misinformation on Social Media</title>
      <link>https://dirkhovy.com/publication/2023-leveraging-social-interactions-detect-misinformation-social-media/</link>
      <pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-leveraging-social-interactions-detect-misinformation-social-media/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)</title>
      <link>https://dirkhovy.com/publication/2023-proceedings-c3nlp/</link>
      <pubDate>Sat, 06 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-proceedings-c3nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers</title>
      <link>https://dirkhovy.com/publication/2023-can-demographic-factors-improve-text-classification/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-can-demographic-factors-improve-text-classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</title>
      <link>https://dirkhovy.com/publication/2023-know-your-audience-education/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-know-your-audience-education/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Beyond Digital &#39;Echo Chambers&#39;: The Role of Viewpoint Diversity in Political Discussion</title>
      <link>https://dirkhovy.com/publication/2023-beyond-digital-echo-chambers-role-viewpoint-diversity-political-discussion/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-beyond-digital-echo-chambers-role-viewpoint-diversity-political-discussion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Viewpoint: Artificial Intelligence Accidents Waiting to Happen?</title>
      <link>https://dirkhovy.com/publication/2023-ai-normal-accidents-waiting-happen/</link>
      <pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2023-ai-normal-accidents-waiting-happen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>It&#39;s Not Just Hate: A Multi-Dimensional Perspective on Detecting Harmful Speech Online</title>
      <link>https://dirkhovy.com/publication/2022-not_just_hate/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-not_just_hate/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data</title>
      <link>https://dirkhovy.com/publication/2022-twitter_demographer/</link>
      <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-twitter_demographer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bridging Fairness and Environmental Sustainability in Natural Language Processing</title>
      <link>https://dirkhovy.com/publication/2022-bridging_fairness_and_environmental_sustainability_in_natural_language_processing/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-bridging_fairness_and_environmental_sustainability_in_natural_language_processing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SocioProbe: What, When, and Where Language Models Learn about Sociodemographics</title>
      <link>https://dirkhovy.com/publication/2022-socioprobe_what_when_where_language_models_learn_about_sociodemographics/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-socioprobe_what_when_where_language_models_learn_about_sociodemographics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages</title>
      <link>https://dirkhovy.com/publication/2022-strategies-hate-speech-detection-under-resourced-languages/</link>
      <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-strategies-hate-speech-detection-under-resourced-languages/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Is It Worth the (Environmental) Cost? Limited Evidence for the Benefits of Diachronic Continuous Training</title>
      <link>https://dirkhovy.com/publication/2022-limitation-diachronic-continuous-training/</link>
      <pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-limitation-diachronic-continuous-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender</title>
      <link>https://dirkhovy.com/publication/2022-welcome_modern_world_pronouns_identity-inclusive_natural_language_processing_beyond_gender/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-welcome_modern_world_pronouns_identity-inclusive_natural_language_processing_beyond_gender/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Guiding the Release of Safer E2E Conversational AI through Value Sensitive Design</title>
      <link>https://dirkhovy.com/publication/2022-guiding_release_safer_e2e_conversational_ai_through_value_sensitive_design/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-guiding_release_safer_e2e_conversational_ai_through_value_sensitive_design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hard and Soft Evaluation of NLP models with BOOtSTrap SAmpling - BooStSa</title>
      <link>https://dirkhovy.com/publication/2022-hard_soft_evaluation_nlp_models_bootstrap_sampling-boostsa/</link>
      <pubDate>Thu, 12 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-hard_soft_evaluation_nlp_models_bootstrap_sampling-boostsa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Invariant Properties in Natural Language Processing</title>
      <link>https://dirkhovy.com/publication/2022-language-invariant-properties-nlp/</link>
      <pubDate>Thu, 21 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-language-invariant-properties-nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benchmarking Post-Hoc Interpretability Approaches for Transformer-based Misogyny Detection</title>
      <link>https://dirkhovy.com/publication/2022-interpretability-transformer-mysogyny-detection/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-interpretability-transformer-mysogyny-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Measuring Harmful Sentence Completion in Language Models for LGBTQIA&#43; Individuals</title>
      <link>https://dirkhovy.com/publication/2022-honest-hurtful-language-model-lgbtqia&#43;/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-honest-hurtful-language-model-lgbtqia&#43;/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pipelines for Social Bias Testing of Large Language Models</title>
      <link>https://dirkhovy.com/publication/2022-pipelines-social-bias-testing-language-models/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-pipelines-social-bias-testing-language-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks</title>
      <link>https://dirkhovy.com/publication/2022-two-contrasting-data-annotation-paradigms-subjective-nlp-tasks/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-two-contrasting-data-annotation-paradigms-subjective-nlp-tasks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>XLM-EMO: Multilingual Emotion Prediction in Social Media Text</title>
      <link>https://dirkhovy.com/publication/2022-xlmemo-multilingual-emotion-prediction/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-xlmemo-multilingual-emotion-prediction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists</title>
      <link>https://dirkhovy.com/publication/2022-entropy-attention-regularization-bias/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-entropy-attention-regularization-bias/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SAFETYKIT: First Aid for Measuring Safety in Open-domain Conversational Systems</title>
      <link>https://dirkhovy.com/publication/2022-safetykit-first-aid-measuring-safety-open-domain-conversational-systems/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022-safetykit-first-aid-measuring-safety-open-domain-conversational-systems/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text Analysis in Python for Social Scientists – Prediction and Classification</title>
      <link>https://dirkhovy.com/publication/2022_nlpss2/</link>
      <pubDate>Sun, 16 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2022_nlpss2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning from Disagreement: A Survey</title>
      <link>https://dirkhovy.com/publication/2021-learning_from_disagreement_survey/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-learning_from_disagreement_survey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Five sources of bias in natural language processing</title>
      <link>https://dirkhovy.com/publication/2021-five-sources-bias/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-five-sources-bias/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Gap between Adoption and Understanding in NLP</title>
      <link>https://dirkhovy.com/publication/2021-gap-between-understanding-adoption/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-gap-between-understanding-adoption/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</title>
      <link>https://dirkhovy.com/publication/2021-contextualized-improve-topic-models-coherence/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-contextualized-improve-topic-models-coherence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>&#39;We will Reduce Taxes&#39; - Identifying Election Pledges with Language Models</title>
      <link>https://dirkhovy.com/publication/2021-aclfindings-mimac/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-aclfindings-mimac/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HONEST: Measuring Hurtful Sentence Completion in Language Models</title>
      <link>https://dirkhovy.com/publication/2021-honest-hurtful-language-model/</link>
      <pubDate>Sun, 06 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-honest-hurtful-language-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Importance of Modeling Social Factors of Language: Theory and Practice</title>
      <link>https://dirkhovy.com/publication/2021-importance-modeling-social-factors-language/</link>
      <pubDate>Sun, 06 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-importance-modeling-social-factors-language/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FEEL-IT: Emotion and Sentiment Classification for the Italian Language</title>
      <link>https://dirkhovy.com/publication/2021-feelit-italian-sentiment-emotion/</link>
      <pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-feelit-italian-sentiment-emotion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MilaNLP @ WASSA: Does BERT Feel Sad When You Cry?</title>
      <link>https://dirkhovy.com/publication/2021-wassa-emotion-multitask/</link>
      <pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-wassa-emotion-multitask/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Universal Joy A Data Set and Results for Classifying Emotions Across Languages</title>
      <link>https://dirkhovy.com/publication/2021-universal-joy/</link>
      <pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-universal-joy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BERTective: Language Models and Contextual Information for Deception Detection</title>
      <link>https://dirkhovy.com/publication/2021_eacl_decour/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021_eacl_decour/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-lingual Contextualized Topic Models with Zero-shot Learning</title>
      <link>https://dirkhovy.com/publication/2021-crosslingual-topic-model/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2021-crosslingual-topic-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text Analysis in Python for Social Scientists – Discovery and Exploration</title>
      <link>https://dirkhovy.com/publication/2020_nlpss/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_nlpss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>“You Sound Just Like Your Father” Commercial Machine Translation Systems Include Stylistic Biases</title>
      <link>https://dirkhovy.com/publication/2020_mt/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_mt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview</title>
      <link>https://dirkhovy.com/publication/2020_bias/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_bias/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visualizing Regional Language Variation Across Europe on Twitter</title>
      <link>https://dirkhovy.com/publication/2020_eutwitter/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_eutwitter/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Helpful or Hierarchical? Predicting the Communicative Strategies of Chat Participants, and their Impact on Success</title>
      <link>https://dirkhovy.com/publication/2020_helpful/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_helpful/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What the [MASK]? Making Sense of Language-Specific BERT Models</title>
      <link>https://dirkhovy.com/publication/2020-bertlang-language-specific-bert/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020-bertlang-language-specific-bert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Case for Soft Loss Functions</title>
      <link>https://dirkhovy.com/publication/2020_aaai_softlabels/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2020_aaai_softlabels/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dense Node Representation for Geolocation</title>
      <link>https://dirkhovy.com/publication/2019_m2v/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019_m2v/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geolocation with Attention-Based Multitask Learning Models</title>
      <link>https://dirkhovy.com/publication/2019_geo_mtl/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019_geo_mtl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hey Siri. Ok Google. Alexa: A topic modeling of user reviews for smart speakers</title>
      <link>https://dirkhovy.com/publication/2019_siri/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019_siri/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Identifying Linguistic Areas for Geolocation</title>
      <link>https://dirkhovy.com/publication/2019_p2c/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019_p2c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Women’s Syntactic Resilience and Men’s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing</title>
      <link>https://dirkhovy.com/publication/2019-gender-bias-part-of-speech-tagging-dependency-parsing/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019-gender-bias-part-of-speech-tagging-dependency-parsing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Peer networks and entrepreneurship: A Pan-African RCT</title>
      <link>https://dirkhovy.com/publication/2019_adansonia/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2019_adansonia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information</title>
      <link>https://dirkhovy.com/publication/2018_emnlp_retro/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2018_emnlp_retro/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting</title>
      <link>https://dirkhovy.com/publication/2018-capturing-regional-variation-distributed-representations-geographic-retrofitting/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2018-capturing-regional-variation-distributed-representations-geographic-retrofitting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparing Bayesian Models of Annotation</title>
      <link>https://dirkhovy.com/publication/2018-comparing-bayesian-models-annotation/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2018-comparing-bayesian-models-annotation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning</title>
      <link>https://dirkhovy.com/publication/2018-predicting-news-headline-popularity-syntactic-semantic-knowledge-multitask-learning/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2018-predicting-news-headline-popularity-syntactic-semantic-knowledge-multitask-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Social and the Neural Network: How to Make Natural Language Processing about People again</title>
      <link>https://dirkhovy.com/publication/2018-social-neural-network/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/publication/2018-social-neural-network/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Some Thoughts on the Future of NLP Conferences</title>
      <link>https://dirkhovy.com/post/2017_09_19_some_thoughts_on_the_future_of_nlp_conferences/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2017_09_19_some_thoughts_on_the_future_of_nlp_conferences/</guid>
      <description>&lt;p&gt;If you have been to ACL in Vancouver or followed the news on Twitter, you know that it was the biggest conference of its kind. And EMNLP, held in Copenhagen in September, could again claim the same for itself, too, with more than 1200 participants and more long paper submissions than ACL. Given the recent interest in all things NLP, this development is not too surprising, and if current trends hold, we should expect further growth and more record attendances to come on a regular basis.&lt;/p&gt;
&lt;p&gt;Personally, I think that is a good thing: there are still plenty of open questions in NLP, and the field as a whole can only benefit if more people devote their thoughts to the questions in our field. However, it will (have to) affect the way we hold conferences.&lt;/p&gt;
&lt;p&gt;While we are still a long way away from conference sizes as seen in medicine and economics, with up to 10,000 participants, we will likely soon regularly reach attendance of 2000 and more participants (as is already happening in the ML community). Even at this level, it is clear that our conference model needs to be open to growth.&lt;/p&gt;
&lt;p&gt;As far as I can tell from my involvement in EMNLP, this growth will affect (at least) three aspects of conferences: reviewing, organization, and structure of the actual conference.&lt;/p&gt;
&lt;p&gt;There has been a lively debate about the state of reviewing in the community (something I like a lot about NLP as a field: people are always willing to tinker with the status quo). Without weighing in on the arxiv debate (which deserves its own discussion), I think it is becoming clear that reviewing is a both a bottleneck and a control mechanism. It is getting ever harder to find reviewers for all submissions, and the quality of the reviews varies a lot. If we reach 2000, 3000, or even 5000 submissions (remember that there will always be way more submissions than papers), we will need enormous PCs to guarantee three reviews. Some people have entertained the idea of open reviewing, but I believe there is a danger that it becomes a popularity contest rather than a fair quality assessment. So far, reviewing is voluntary, but thankless, despite attempts at reviewing prizes. One incentive could be the requirement to review if you submit: since most papers have more than one author, this could go a long way to reducing the reviewer problem. It would not address quality, but it is hard to see how to tackle this better.&lt;/p&gt;
&lt;p&gt;Even if we solve the reviewer problem, we are still left with the decision how to accept papers: right now, conferences aim for 20-25% accepted papers per area. To me, this is the crucial measure to control the ultimate conference size. If we keep the current ratio and submissions keep growing, so will conference sizes. The alternative is to set a size limit instead: the top N papers (by score, meta-review, and random tie break) across all areas get in, the rest is rejected. The problem with this approach is of course that it will drop acceptance rates precipitously, and make it so much harder for good work to get published, but it would allow us to have an upper bound on the size.&lt;/p&gt;
&lt;p&gt;As for organization, I do enjoy the personal touch that the involvement of community members as general, local, publication, and other chairs brings to conferences. However, from my own experience I can say how hard it is to combine it with your day job as researcher, and once we pass the 2000 mark, it will become almost prohibitive. Given how tight a schedule many researchers already have, this does not sound like a feasible way forward. Certain positions should always be held by researchers, of course (program and are chairs, for example), but it should be possible to outsource some of the other positions.&lt;/p&gt;
&lt;p&gt;A relatively straightforward solution to this problem would be another full-time position in the ACL exec, to support the people who are already there, and to professionalize certain responsibilities currently held by researchers (for example local, handbook, or publicity chair). The increase in salary costs for ACL would most likely be offset by the increased participant numbers.&lt;/p&gt;
&lt;p&gt;This would help reduce variability: none of us are trained conference organizers, and since it is always somebody new picking it up as they go, outcomes vary quite a bit. The permanent members of ACL provide guidance and continuity, but while they are doing an outstanding job, they too feel the brunt of increasing conference sizes (probably even more than others). An additional position would help address this. Fewer organizers would help streamline communication.&lt;/p&gt;
&lt;p&gt;Lastly, larger attendance numbers will change the actual structure of the conferences as we know them. We might have to envision a completely new model, where conferences become more of a discussion forum to exchange ideas than a presentation medium.&lt;/p&gt;
&lt;p&gt;Increasingly, the parallel track model is reaching its limits (although parallel poster sessions seem to work well), and we will either have to introduce even more parallel sessions (unpopular, since attendees will have more conflicts of presentations scheduled at the same time), extend the conference duration (also unpopular, especially for members with family and small kids), or shorten talks. Personally, I am for a 5min talk limit: as it is, most talks can not convey all of the information contained in the paper anyway, so there is little reason for them to be so long. The best a talk can do is serve as appetizer for the paper, which you then want to read in your time. That, however, can be accomplished in 5min just as well as in 12 or 15. I think the exchange in QA sessions is great and should be kept as much as possible, but I am not sure it will be feasible. A better exchange is the direct chat during poster sessions, so increasing poster acceptances is an obvious solution (although maybe not an easy one). Personally, I increasingly like the focused topicality and exchange of workshops, but I am not sure it will be possible to scale them (even at an average workshop size of 50 participants, we would have to have dozens of workshops, which again raises the time vs. parallelity problem, not to mention space issues).&lt;/p&gt;
&lt;p&gt;Either way, the growing field will present us with new challenges and affect the way we do conferences. However, I am confident that we will find a way as a community, and am more curious than concerned. The only thing that is for certain is that conferences as we know them will become a thing of the past.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It’s All About the He Said, She Said―A Quantitative Analysis of the Three Presidential Debates</title>
      <link>https://dirkhovy.com/post/2016_10_22_its_all_about_the_he_said_she_saida_quantitative_analysis_of_the_three_presidential_debates/</link>
      <pubDate>Sat, 22 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2016_10_22_its_all_about_the_he_said_she_saida_quantitative_analysis_of_the_three_presidential_debates/</guid>
      <description>&lt;p&gt;It’s All about the He said, She said―A Quantitative Analysis of the Three Presidential Debates&lt;/p&gt;
&lt;p&gt;The question what constitutes an acceptable sentence is a matter of taste, and would elicit very different answers from a moral philosopher, a linguist, and a logician. In natural language processing (NLP), the answer is much simpler (or simplistic), and quantifiable: any sentence that could be generated with some probability from a language model.&lt;/p&gt;
&lt;p&gt;As the US presidential debates have drawn to a close, much has been said about acceptable and unacceptable language. While NLP is woefully ill-equipped to make moral decisions on what the candidates said, it is pretty useful to analyze how much was said, and how unusual it is. So I spent an afternoon analyzing the transcripts of the three debates, and quantified the findings.&lt;/p&gt;
&lt;p&gt;I downloaded the three transcripts, separated out the answers of the two candidates, split them into sentences, and analyzed them with a language model. Without going into to much technical detail: a language model is a statistical model which has been induced from a large collection of text. To use the model, we give it a sentence and ask  “How likely is it to generate this sentence?” The model then returns a probability between 0 and 1. 0 means that the model would never produce this sentence, and 1 that it always would. In practice, neither of them really occur, but numbers are somewhere in between.&lt;/p&gt;
&lt;p&gt;The exact numbers depend on what and how much text you used to train the model, how many words in sequence you look at, and how similar your training data was to the texts you analyze. I used a 5-gram SRILM trained on a corpus of 2,584,929 English review sentences. No, this might not be the best model one could use, and if I used it in an application, I would certainly train on something closer to the debates. However, I used the same model for all three debates and both candidates, so independent of the absolute values we get, we can compare the two politicians quantitatively.&lt;/p&gt;
&lt;p&gt;So what do we learn?&lt;/p&gt;
&lt;p&gt;First of all, Donald Trump says more (1950 sentences, compared to Clinton’s 1136), but he uses fewer words: the median Trump sentence has 11 words, a Clinton sentence 16. The graph below shows the relative distribution of sentence lengths for each candidate (I accounted for the fact that they uttered different amounts of sentences).&lt;/p&gt;
&lt;p&gt;Because the bars are sometimes a little hard to see in front of each other, I also overlaid them with a smoothed curve (kernel density estimator). The dotted lines show the respective median length in words.&lt;/p&gt;
&lt;p&gt;We can see that Trump utters more short sentences (under 15 words), and few longer sentences. Clinton, on the other hand, has a lot more of her sentences in the 15-30 word range.&lt;/p&gt;
&lt;p&gt;What about the language model? Let’s first look at the likelihood of the sentences. Some explanation: since the probabilities get very small and hard to distinguish, the likelihood of the sentence is typically given as logarithm of the probability. That makes it a larger, but negative number. The closer the number is to 0, the more likely a sentence is under the model.&lt;/p&gt;
&lt;p&gt;We again see some noticeable differences: Trump’s sentences are usually more likely than Clinton’s. This is both an effect of the words the two use, but also of the sentence length (longer sentences become less and less likely), and we have already seen that there are noticeable differences in sentence length.&lt;/p&gt;
&lt;p&gt;So let’s normalize each sentence likelihood by the sentence length. That gives us the average log probability per word (note that the x-axis scale is much smaller than before).&lt;/p&gt;
&lt;p&gt;Even here, on a per-word-basis, we see that the model is more likely to produce Trump sentences rather than Clinton sentences (you can actually use language models to generate sentences, often to great comical effect, but there isn’t enough training data for each candidate to really come up with much. I tried).&lt;/p&gt;
&lt;p&gt;So what do the different sentences look like? Well, the two highest scoring sentences (measured by logprob/word) for each candidate are  “Because I was a senator with a Republican president .” (Clinton) and  “Horribly wounded .” (Trump). The most  “average” sentences are  “But let ’ s not assume that trade is the only challenge we have in the economy .” (Clinton) and  “When we have $ 20 trillion in debt , and our country ’ s a mess , you know , it ’ s one thing to have $ 20 trillion in debt and our roads are good and our bridges are good and everything ’ s in great shape , our airports .” (Trump). Both of these buck the length-trend. The least likely sentences of each candidate, however, do follow what we have seen before:  “Donald thinks belittling women makes him bigger .” (Clinton) vs.  “Trump Foundation , small foundation .” (Trump).&lt;/p&gt;
&lt;p&gt;So independent of what the candidates are talking about, the way how they talk can help us separate them to some extent. In fact, if we use only the number of words and logprob as features, we can train a logistic regression classifier that distinguishes the two candidates with an accuracy of over 65% (10-fold cross-validation). That’s only slightly better than the majority class (about 63% accuracy) and again not good enough to build a system, but interesting given that we have not even looked at what the candidates are saying.&lt;/p&gt;
&lt;p&gt;Does this tell us anything about the likely outcome in November? No. But it shows that the differences between the candidates’ rhetoric styles go beyond what they say in a quantifiable way: sentence length and predictability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Science’s genius complex</title>
      <link>https://dirkhovy.com/post/2016_07_06_sciences_genius_complex/</link>
      <pubDate>Wed, 06 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2016_07_06_sciences_genius_complex/</guid>
      <description>&lt;p&gt;In a recent article in the New Yorker, James Surowiecki outlined how, back in the 1960s, professional athletes considered strength training akin to cheating: either you were good at sports, or you weren’t―training had nothing to do with it. Practice was seen as just a way to stay in shape, not to get better. Today, this notion sounds quaint, naive, and a little bit stupid. We expect professional athletes (and any remotely serious amateurs) to have a rigorous training regimen, including fitness, nutrition, and rest schedules.&lt;/p&gt;
&lt;p&gt;When it comes to scientists, however, we still think along the same lines as the athletes in the last century. Da Vinci, Einstein, Curie: We like to think that these people had an innate  “gift”, a knack for science, that they were just brilliant and needed no training. Yes, they were extremely smart, but nothing could be further from the truth. Da Vinci and Tesla had sophisticated sleep schedules to maximize efficiency, Einstein arranged his personal life around his science (with no regard for the people around him), and Curie literally worked herself to death.&lt;/p&gt;
&lt;p&gt;The notion of inherent brilliance, however, does not only pertain to the all-time greats. Scientists are generally portrayed as geniuses, who possess a preternatural insight and operate on a different plane from mere mortals. This is an overcome and elitist notion, and to perpetuate that stereotype is not only disrespectful to all the hard-working researchers, but also damaging to science.&lt;/p&gt;
&lt;p&gt;Every great academic I know has indeed an unusually good understanding of their subject matter, but mostly, they do because they work a substantial amount. And the more accomplished they are, the more they work. None of them could get by on talent alone, neither to get where they are, nor to maintain that level.&lt;/p&gt;
&lt;p&gt;There are of course no well-known training regimen for scientists (What is the equivalent of endurance training for researchers? How should one eat and rest to achieve maximum performance?)&lt;/p&gt;
&lt;p&gt;However, many researchers I know exercise regularly, both to maintain their health, and as counterbalance to their academic routine. And while not many academics eat an athlete’s diet, many of them follow scrupulous caffeination rituals.&lt;/p&gt;
&lt;p&gt;More importantly, though, the best researchers are constantly finding ways to identify and improve their weaknesses. And the only way to do so is by investing time. Lots of time.&lt;/p&gt;
&lt;p&gt;A job at a prestigious university these days comes with the implicit understanding (from both sides) that you put in 80+ hours a week, not necessarily that you are brilliant. Work-life balance be damned.&lt;/p&gt;
&lt;p&gt;This approach has some serious side effects, with alcoholism and burn-out unusually common among academics. Yet we still try to make it look easy on the outside, slave to the genius fallacy. We hope to convince people of our brilliance, while simultaneously fighting back the impostor syndrome, and wondering how the others do it so effortlessly. Truth is: they don’t.&lt;/p&gt;
&lt;p&gt;This is even more infuriating since academia is already set up as a series of escalating training rounds, and would benefit from acknowledging that. The genius complex is holding us all back, devalues hard work, and makes it difficult for young researchers to accept their limits, to acknowledge that their accomplished colleagues got to where they are by years of hard work and scrupulous training, rather than by mere natural talent.&lt;/p&gt;
&lt;p&gt;Sports and music have abandoned the genius notion in favor of dedicated training and hard work, and consequently, performance has improved across the board over the last few decades. And while the overall quality of science has improved by the same mechanism, we still cling to an overcome notion that brings more harm than good. It’s time we abandon it as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fun with Movie Titles</title>
      <link>https://dirkhovy.com/post/2015_09_18_fun_with_movie_titles/</link>
      <pubDate>Fri, 18 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_09_18_fun_with_movie_titles/</guid>
      <description>&lt;p&gt;As mentioned before, I sometimes use my academic knowledge of natural language processing for purposes other than research.&lt;/p&gt;
&lt;p&gt;A friend recently told me about an entertaining game where you gerundivize words in movie titles (i.e., you add -ing at the end) to completely change the meaning. Some of my PG-13 favorites include  “Jurassic Parking”,  “Ironing Man” and  “2001: Spacing Oddysey” (you can get equally entertaining, yet NSFW result with other titles, but I leave that as an exercise to the reader).&lt;/p&gt;
&lt;p&gt;Being the spoilsport I am, I decided that it would be fun to see how much NLP could help me with that. It certainly won’t be able to decide whether an altered title is funny or not (that’s how lame AI still really is), but it would at least help me generate all possible versions.&lt;/p&gt;
&lt;p&gt;I downloaded a list of the 1000 best movies ever (at least according to The New York Times) and then took each title, went through it word by word, and checked (against the Brown corpus), whether adding ’-ing’ to the end resulted in an English word. If so, I printed it out.&lt;/p&gt;
&lt;p&gt;The only tricky part was to deal with the spelling alterations for gerunds: -e is always removed ( “take” becomes  “tak-ing”), consonants are usually doubled ( “put” becomes  “put-t-ing”, but  “model” becomes  “model-ing”). For the latter case, I actually generate both the duplicated and a non-duplicated version (the rules for the game are not quite clear on what to do here). That’s how I got  “Beverly Hills Coping”, which I think sounds much funnier than  “Beverly Hills Copping”.&lt;/p&gt;
&lt;p&gt;I didn’t check for grammaticality of the entire title, which results in nonsense such as  “Alice Doesn’t Living Here Anymore”, while other results are just minor changes in meaning ( “Dial M for Murder” vs.  “Dialing M for Murder”). Some of the results are pretty entertaining, though:  “The Counting of Monte Cristo”,  “Lasting Tango in Paris”,  “Gosford Parking”,  “Gone with the Winding”,  “Lone Staring” (creep!),  “Odd Man Outing”,  “Oliver Twisting”,  “Totaling Recall”, or  “Body Heating”. You can download the script here and play around with it to get the full list, or modify it with your own titles.&lt;/p&gt;
&lt;p&gt;And that’s it, I wasted another perfectly good hour using NLP for my personal entertainment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How usable is sentiment analysis?</title>
      <link>https://dirkhovy.com/post/2015_09_18_how_usable_is_sentiment_analysis/</link>
      <pubDate>Fri, 18 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_09_18_how_usable_is_sentiment_analysis/</guid>
      <description>&lt;p&gt;Recently, I was interviewed by two students for a study on the business application of natural language processing technique called sentiment analysis. Sentiment analysis takes as input a text (which can be anything from a sentence, a tweet, or a paragraph, up to an entire document), and tries to predict the general attitude expressed therein: usually divided into positive, negative, or neutral.&lt;/p&gt;
&lt;p&gt;For many businesses, this is an appealing application, since it promises to detect how people think about the company’s products and services, and because it can potentially be used to evaluate stock options.&lt;/p&gt;
&lt;p&gt;However, potential and reality differ, and as far as I see it, there are currently three problems that limit the general applicability of sentiment analysis, and their commercial use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The labels:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The labels are fairly coarse (positive, negative, neutral), while there is still an ongoing debate in psychology on how many basic emotions there are (see here). More fine-grained labels (Facebook let’s you label your status with more than 100  “emotions”) might provide better leverage, but the question is: what would they be? Another problem is the relation between text and labels: we recently had a paper accepted which shows that a common approach to labeling (by using ratings) is not strongly correlated with the text, i.e., models (and humans) can’t guess correctly how many stars somebody gave, only based on the review text. This is largely what we expect of our statistical approaches, though. Which brings us to the second problem:&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The models&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The models are usually trained on a particular domain (say, movie reviews), where they learn that certain features are indicative, say for movies the word ’hilarious’. However, when applied to another domain (say, restaurant reviews), this word does not at all indicate positive sentiment (a  “hilarious” meal might not be what we hope for).&lt;/p&gt;
&lt;p&gt;In technical terms, models overfit the training data. For a negative result on this, see here. Models need to be better regularized, i.e., de-biased from the training data, in order not to put too much faith in spurious features. Which finally brings us to the third problem:&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The features&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The problem of most approaches is the reliance on individual words, rather than on global sentiment and a deeper understanding of the text. Many models still rely on predefined word lists, or dictionaries, but individual words do not do the problem justice. Things like negation, sarcasm, or metaphors can completely distort the sense of a phrase, even though the individual words seem unambiguous. Even seemingly clear positive or negative words can often express both sentiments when seen in context, cf.  “sincere” in  “sincere answer” vs.  “sincere condolences”, or  “cold” in  “cold look” vs.  “cold beer” (see Flekova et al.). This doesn’t even begin to cover the problem that different age and gender groups express positive or negative sentiment very differently, yet that the models treat all text as coming from the same demographics.&lt;/p&gt;
&lt;p&gt;In sum, our approaches are currently too simplistic to capture the complexity of entire texts, thus making results brittle. The over-reliance on individual words and the lack of model regularization exacerbate this problem.&lt;/p&gt;
&lt;p&gt;This is not to say that sentiment analysis does not work at all, but all of this limits the commercial use of sentiment analysis to fairly clearly denominated domains (see also the assessment of Larson and Watson).&lt;/p&gt;
&lt;p&gt;To improve make sentiment analysis viable for a wider range of contexts, though, we have to start improving all of the three areas above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wow: Such Meme, Much NLP, Very Generate!</title>
      <link>https://dirkhovy.com/post/2015_09_01_wow_such_meme_much_nlp_very_generate/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_09_01_wow_such_meme_much_nlp_very_generate/</guid>
      <description>&lt;p&gt;I love natural language processing, I really do. I think it has the potential to make the world a little bit better, and like all things worth exploring, it also has the potential to do evil. But it can be tiring wielding all this awesome technology for such serious causes, and sometimes, a man just wants to have a little fun with his subject. After all, why not get a laugh out of all the time invested?&lt;/p&gt;
&lt;p&gt;Turns out, you can have a lot of fun with NLP, and it usually only takes a few lines of code and some data. An internet age ago (i.e., last year or so), the Doge meme took the Web by storm, and after seeing it enough, I realized that it followed a certain pattern, and that much of the humor derived from the ungrammatical use of intensifiers with certain word types (I’m a lot of fun at parties, I swear). Essentially, the pattern is&lt;/p&gt;
&lt;p&gt;“Wow, such ADJECTIVE! Much NOUN! Very VERB!”&lt;/p&gt;
&lt;p&gt;All I had to do now was to get all nouns, verbs, and adjectives out of an annotated corpus (I used the Brown corpus, which comes with the NLTK library), and then randomly pick one of each category to fill the slots. Oh, yeah, and I converted all verbs to their infinitive (i.e,. ’jumped’ and ’jumping’ become ’jump’).&lt;/p&gt;
&lt;p&gt;The results range from the mundane and stupid to the insightful and funny. My favorites are the jaded look on counter culture in  “Wow, such underground! Much intentions! Very smell!” and the strangely place-appropriate  “Wow, such scandinavian! Much concrete! Very constitute!”&lt;/p&gt;
&lt;p&gt;And that’s it. The entire script is 6 lines of Python code. You can download the script here and play around with it. Enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are our models ageist?</title>
      <link>https://dirkhovy.com/post/2015_07_10_are_our_models_ageist/</link>
      <pubDate>Fri, 10 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_07_10_are_our_models_ageist/</guid>
      <description>&lt;p&gt;A number of comedies hinge on the premise of a young and an old person switching bodies. When a 45-year-old business woman says  “’sup dudes?”, we find this funny (at least some of us), because it goes against our expectations of how people speak. We do have fairly clear ideas of how 45-year-old business women speak, and how teenage guys do, and that these two ways are not the same. To us, this is such a common and intuitive fact about language that breaking it intentionally can have comedic value.&lt;/p&gt;
&lt;p&gt;For the models we use in natural language processing (NLP), however, this fact is not at all clear. To them, all language is the same, because we have not taught them about the difference. When I say  “taught”, I don’t mean that we sat them down and explained how language works, of course. We train a model by presenting it with a bunch of input sentences, and with the correct output analyses we expect for them. If the machine has seen enough of these input and output pairs, it can learn a function that maps from an input sentence to the output analysis.&lt;/p&gt;
&lt;p&gt;The problem is that almost all of these training pairs came from newspaper articles from the 80s. And that most of these articles were produced by (and for) a specific demographic, which is, broadly speaking, old, white, and male.&lt;/p&gt;
&lt;p&gt;We would expect 60-year-old men to have difficulties understanding  “the kids these days”, and so it’s no surprise that our models have exactly the same difficulties. When we give them input sentences from today’s teenagers (e.g., Twitter), they produce incorrect analyses. Of course, tweets are written very differently from newspaper articles, and so for a while now, the field has investigated the influence of the genre on performance. However, genre is not the whole picture: if we feed the models sentences from older people, they do a lot better than on sentences from younger people, even when the genre is the same for both groups.&lt;/p&gt;
&lt;p&gt;Again, this is not too surprising, since language changes with every generation. What is surprising, however, is the depth and magnitude of this change. Younger people do not just use different words than older people. If it was that simple, we could just have the machine learn a number of new words. It turns out, however, that the differences go deeper: younger people even put their words together in ways very different from older people.&lt;/p&gt;
&lt;p&gt;In order to test this, we ignored the actual words and looked instead at pairs of the words’ parts of speech (noun, verb, adjective, etc.) So  “Dan cooks”  “Mary writes” or  “Frank measures” all become  “NOUN VERB”. We found that in both German and English, the pairs from the older group were much more similar to the training data than the pairs of the younger group were. In other words: younger people use word combinations that are unlike anything our models have seen before. Older people don’t. Consequently, our models are much better at predicting part of speech sequences for the older group. We tested this for both German and English, with the same results.&lt;/p&gt;
&lt;p&gt;Pairs of parts of speech are one thing, but linguistically speaking, they are still a fairly shallow phenomenon.&lt;/p&gt;
&lt;p&gt;We also looked at the deep syntactic structure of grammatical functions (subject, verb, object, etc.), where words do not have to be adjacent, but can be on opposite ends of the sentence.&lt;/p&gt;
&lt;p&gt;These analysis were interesting in two ways: from a linguistic perspective, and from an NLP perspective.&lt;/p&gt;
&lt;p&gt;Linguists have suspected for a long time that syntax changes with age. However, since syntax is very complex, this was hard to prove: we can put words together in a great number of ways, and you have to observe lots and lots of examples to see even the most common ones. Even then, it is hard to pin down the exact differences, if you don’t know what constructions you are looking for. We got around both problems by analyzing millions of sentences from people whose age we know. Among those, we selected only the most frequent syntactic constructions and compared them. That way, we did not have to specify beforehand which constructions to look for. The pattern analyses was of course less than perfect (remember, our models are biased), but by analyzing large enough numbers and by focusing on frequent constructions, we were able to get enough reliable observations to find significant differences. We expect the differences to be even more pronounced if the analyses were better.&lt;/p&gt;
&lt;p&gt;The result of all this is that even the word-order (syntax) of young people is radically different from the older group. So different, in fact, that seeing a certain construction can give the machine a good clue as to how old the person is. Just as it would us humans.&lt;/p&gt;
&lt;p&gt;This does of course not mean that one group uses a syntactic construction and the other group doesn’t. It just means that one group uses a construction statistically significantly more often than the other group.&lt;/p&gt;
&lt;p&gt;And the differences don’t just extend to age: we found similar differences again between men and women. What’s even more startling is the fact that these patterns occur in up to 12 different indo-european languages.&lt;/p&gt;
&lt;p&gt;The other way in which these findings were interesting, namely for NLP, was that it showed that our models do pick up on demographic differences, albeit in a bad way. There is, however, nothing inherently ageist to the model algorithms: they are not consciously aware of these differences. They simply transform input sentences into output analyses. However, due to their training, they pick up on the language characteristics of the training data. And when the models get new inputs, they expect the language to be the same as before. This shows how brittle our models are, and how susceptible to the language characteristics (the bias) in the training data.&lt;/p&gt;
&lt;p&gt;In fact, we found that our models not only did consistently worse on data from young people (in both German and English), but that they also performed worse and worse the more markers of African-American vernacular English (AAVE) were in a text. (They did not, however, perform worse on different genders―at least.)&lt;/p&gt;
&lt;p&gt;So you got a bunch of bad analyses, you could say―so what!&lt;/p&gt;
&lt;p&gt;Indeed, if it was just for academic purposes, this would be annoying, but on the whole inconsequential. However, NLP models are increasingly used as go-to tools for unstructured data analysis, both in business and political analysis. If all of these models expect language to come from old white men, and then perform poorly on texts from other demographic groups, we risk systematically ignoring or, even worse, disadvantaging these groups.&lt;/p&gt;
&lt;p&gt;Luckily, there are ways to prevent this problem. For one, we can simply train our models on data from more and more demographic groups. In a recent paper, I showed that if we encode age and gender in the models, we get better performance, even under very controlled settings. This means that there is enough signal in the language of different demographic groups that our models can learn to differentiate, and to produce better analyses on a variety of tasks and languages.&lt;/p&gt;
&lt;p&gt;This requires, though, that we have enough samples from all demographic groups, and their correct analyses. Both assumptions are unrealistic (for now), because collecting the data and producing the correct analyses takes a lot of time and effort. What’s more, there are dozens of demographic variables: age, gender, education, ethnicity, class, income, etc., and we are only starting to see which ones impact our models.&lt;/p&gt;
&lt;p&gt;If we want to address the problem in earnest, we can’t afford to encode each of these variables explicitly. However, we can also just tell our models to expect demographic differences, and figure out the rest themselves.&lt;/p&gt;
&lt;p&gt;In the future, we need to find ways to automatically detect all kinds of variations, and to reduce the impact of them on training our models. We need to teach our models that language varies along demographic lines, but that all of these variations are valid.&lt;/p&gt;
&lt;p&gt;Not only will we improve the quality of our models, we will also produce fairer analyses that benefit everyone the same.&lt;/p&gt;
&lt;p&gt;The papers I described can be found here, here, here, and here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What I do: Learning whom to trust</title>
      <link>https://dirkhovy.com/post/2015_01_11_what_i_do_learning_whom_to_trust/</link>
      <pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_01_11_what_i_do_learning_whom_to_trust/</guid>
      <description>&lt;p&gt;Here, I’ll try to explain another paper I have worked on in generally understandable terms. This time, it’s about learning whose opinion we can trust.&lt;/p&gt;
&lt;p&gt;Say you are thinking of watching  “Interstellar”. You have seen mixed reviews and want to poll your friends for an opinion. So you ask 6 people. If five say  “don’t bother” and one  “yay”, it’s pretty clear. This is called  “majority voting”.&lt;/p&gt;
&lt;p&gt;However, more often than not, you will get a tie, i.e., three people say  “don’t bother” and three  “yay”. This gets worse the more answer options there are: imagine asking six people whether to see  “Interstellar”,  “The Hobbit”,  “Fury”, or  “Unbroken”.&lt;/p&gt;
&lt;p&gt;One way to break ties is by flipping a coin (or rolling a die, if you have more than two answer options). However, this has a 50% or higher chance of picking the wrong answer.&lt;/p&gt;
&lt;p&gt;If you knew, however, that one of your friends simply likes all movies and always says yes, and another one has the same taste as you and said no, you would be able to weigh their answers differently. Instead of flipping a coin to break ties, you’d use a weighted average to get the best answer.&lt;/p&gt;
&lt;p&gt;In Natural Language Processing, we often ask people for their opinion. Usually, it’s about the part of speech (noun, verb, adjective, etc) of words, or whether a sentence is positive, negative, or neutral. This is called annotation. The annotated text is then used to train statistical models. If the annotation is wrong, the models won’t work as well.&lt;/p&gt;
&lt;p&gt;We assume that most annotators are generally trustworthy, but that some annotators get it wrong. Either because they did not pay attention to the explanation, or because they don’t care about the task and just want to get paid. If we could down-weigh bad annotators, we would get better annotations and thus better statistical models.&lt;/p&gt;
&lt;p&gt;Unfortunately, we usually don’t know the annotators, and thus we don’t know how much to weigh each annotator’s answer. If we did, we could just compute a weighted average over the annotators and get the most likely correct answer.&lt;/p&gt;
&lt;p&gt;If we already knew the correct answers, we could count how often each annotator gave the correct answer, and use that fraction as weight.&lt;/p&gt;
&lt;p&gt;But we don’t know the correct answer, and we don’t know the weights, so we are stuck in a circular problem.&lt;/p&gt;
&lt;p&gt;The way to address this circular problem is by using an algorithm called Expectation Maximization (EM). It works in two steps that are repeated until we reach a satisfying answer.&lt;/p&gt;
&lt;p&gt;Initially, we give each annotator some random weight. In the first step, we then calculate the most likely answers based on the weights. Now we can compute how many answers each annotator got right, and assign them a weight based on that fraction. This is the second step.&lt;/p&gt;
&lt;p&gt;With the new weights, we re-calculate how many answers each annotator gets right, and again update the weights. And again, and again. At some point, the weights don’t change much any more from round to round, and we are done. We now have weights and can compute the most likely final answers.&lt;/p&gt;
&lt;p&gt;We also use an additional technique, called Variational Bayes EM, that essentially tells the model that people either do a good job or they don’t care, but nobody cares a little. This is called a  “prior belief”, or just  “prior”. Technically, this works by adding pseudo-counts, i.e., when computing how many answers each annotator got right and wrong, we add a small number (we used 0.5) to both. The reason why this works is complex, but it essentially relativizes the influence of the counts a bit, unless they are pretty strong. In the end, it prevents the model from giving too low weights to actually good annotators and improves performance.&lt;/p&gt;
&lt;p&gt;Using the final weights for each annotator, we can compute a likelihood for each answer under the model (i.e., given that particular set of weights, how likely is a particular answer). The product of all answer likelihoods gives us a measure for how good the model is. Ideally, we would like to have a model with high likelihood.&lt;/p&gt;
&lt;p&gt;Since we started out with random weights, there is a chance that a bad annotator ends up with a high weight. It’s a small chance, because the training process tends to correct that, but it exists. To eliminate that chance, we run the training several times, every time with different starting weights. Once a training run finishes, we measure the overall likelihood of the model. We then pick the final set of weights that resulted in the highest overall likelihood.&lt;/p&gt;
&lt;p&gt;We implemented all this in a model and called it MACE, which stands for Multi-Annotator Competence Estimation, because a) that describes what it does and b) we thought it sounded funny to say  “Learning whom to trust with MACE” (yes, this is how scientists work).&lt;/p&gt;
&lt;p&gt;When we tested MACE on data sets where we already knew the correct answer, we found that MACE correctly finds more than 90% of the answers, while majority voting (with coin flipping to break ties) does much worse.&lt;/p&gt;
&lt;p&gt;In real life, we of course don’t know the correct answers, but we found in several annotation projects that the MACE answers produce better statistical NLP models than when using majority voting annotations. We also found that annotators who get a low weight usually also produce bad work, while the ones with high weights produce good annotations.&lt;/p&gt;
&lt;p&gt;Since we have probabilities for each answer, we can also choose to focus on the ones with high probabilities. If we do that, we see that the accuracy for those answers is even higher than for all. This is interesting for the case where we have some more annotations than we need, but would like to know that the ones we choose are of especially high quality.&lt;/p&gt;
&lt;p&gt;When asking people to annotate, we can also smuggle test questions in there where we know the correct answer. These are called control items (because we can control how good the annotators are). That way, we can sort out bad apples even more accurately. If we use even just a few control items in MACE, accuracy goes up even further.&lt;/p&gt;
&lt;p&gt;When I gave a talk about MACE, one of the listeners asked what would happen if my annotators were a bunch of monkeys: would MACE still find the  “experts”? The answer is no, but it’s a good question, and we actually did test how many  “good” annotators the model needs to find good answers. We simulated 10 annotators and varied the number of good ones: those would get 95% of the answers correct (this is roughly the percentage of the best annotators in real life). The rest of the simulated annotators would pick an answer at random or always choose the same value. We found that even with just 3 or four good annotators, MACE was much better in recovering the correct answer than majority voting. Luckily, in real life, it is pretty unlikely to have such a bad crowd of annotators. Just don’t use monkeys.&lt;/p&gt;
&lt;p&gt;Whether we have control items or not, we can use MACE to get better answers for our annotation projects, and learn in the process which annotators are doing a good job.&lt;/p&gt;
&lt;p&gt;The paper I explained here is this one, and MACE can be downloaded here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What I do: Significance Testing</title>
      <link>https://dirkhovy.com/post/2015_01_10_what_i_do_significance_testing/</link>
      <pubDate>Sat, 10 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2015_01_10_what_i_do_significance_testing/</guid>
      <description>&lt;p&gt;As much as I love languages, one of the things that frustrated me in linguistics was the seeming arbitrariness of theories. There was no way of knowing which one was better than another. That did not stop people from arguing about exactly that, but there was no way of proving it.&lt;/p&gt;
&lt;p&gt;One of the things that most drew me to natural language processing was the possibility to measure and quantify how good a model (and thereby its underlying linguistic theory) was. I was overjoyed. Unfortunately, nothing is that easy.&lt;/p&gt;
&lt;p&gt;It turns out that the closer you look, the more difficulties there are. However, there are also solutions. One of them is significance testing. It’s very powerful, but very easy to misunderstand.&lt;/p&gt;
&lt;p&gt;While it is easy to compare two models A and B on the same data set and decide which one is better, this says very little about which model is better in general. Model B might be better on this particular data set, but bad on all others (this is called overfitting). We can get a better picture if we compare the two models on more data sets and average over them. Most likely, however, the difference between two good models will be small.&lt;/p&gt;
&lt;p&gt;So even if we used several data sets, there is still a chance that the difference between A and B is just due to some unaccounted peculiarities, or pure coincidence. This chance gets smaller with more data, but it does not disappear. Can we quantify that chance? This is what significance tests are for.&lt;/p&gt;
&lt;p&gt;In general, significance tests estimate the probability that the claim that the difference between the models is not coincidence is false. I.e., how likely am I wrong when I say  “the difference is not due to chance”. This probability is the p-value. A p-value of 0.01 thus means: even though we have shown that the difference between the models is not coincidence, there is a 0.01=1% chance that we were wrong. If our significance test value is lower than this 0.01, then we can say that the difference is  “statistically significant at p=0.01”.&lt;/p&gt;
&lt;p&gt;Naturally, the lower the p-value, the better. The important point is that significance is binary: either your result is significant at a certain p-value or it isn’t. This is why this list of descriptions for failed significance tests is rather hilarious.&lt;/p&gt;
&lt;p&gt;Ok, great. So does that mean if I see a significant result at a small p-value in a paper, the model is good? Unfortunately, no. Because there are a lot of things that can influence the p-value. Here are some.&lt;/p&gt;
&lt;p&gt;The most obvious is the test we use. Some tests only work if your data is normally distributed, i.e., if you plot the data, it looks like a bell shape. This is almost never the case in language. Most data looks like a Zipf curve, i.e., it has a steep decline and then a long tail. Any test that makes the normal-distribution assumption is thus bound to give a wrong result.&lt;/p&gt;
&lt;p&gt;A good significance test to compare two models is bootstrap sampling: pick a random sample from the data (instances can be repeated) and compare the two models on that. Do this 10.000 times or so. Count how often B is better than A and divide that by 10.000. That’s your p-value. If the result is small, A is probably a better model.&lt;/p&gt;
&lt;p&gt;It does not matter how your data is distributed, this gives us a good estimate.&lt;/p&gt;
&lt;p&gt;Ok, so are we done now that we have a good test? Again, no. There are more factors, and even if we pick a certain p-value threshold and report significance, we could be wrong.&lt;/p&gt;
&lt;p&gt;Say my models analyze sentences. Maybe I need to restrict my analyses to short sentences (say, less than 20 words) for computational reasons. If A does better than B on this sample, I still have no idea whether it will also be better on longer sentences. My significant result is thus only valid for sentences shorter than 20 words. Unless I say this explicitly, my significant result is misleading. If I wanted to deceive people into thinking my model is great, I could look at different lengths and choose to just report the one that gives me a significant result.&lt;/p&gt;
&lt;p&gt;Another issue is the measure I use to compare the models. When analyzing the performance of two models on sentences, I can look at how many sentences each gets right, or at how many words. Or I can just look at verbs. Or rather than the correct items, I can look at the error rate of a certain category. Or a whole number of other measures. All of these can be interesting, but if I get a significant result for one measure, it does not mean I get a significant result for all the others. If I was an unscrupulous researcher, I could test all measure and then just report the ones that look best.&lt;/p&gt;
&lt;p&gt;Typically, the larger the data and the bigger the difference, the easier to get a low p-value. Somewhat counterintuitively, this does not mean that increasing the sample size will give a significant result. Maybe I just add more examples where A and B are the same, or more where the weaker model is stronger, and so the differences wash out.&lt;/p&gt;
&lt;p&gt;Ultimately, all that a positive significance test can tell us is that the difference between models for this particular data set, under the given conditions, for the given measure is significant at a certain level. That’s a lot of qualifications.&lt;/p&gt;
&lt;p&gt;The best we can do under these circumstances is to use several data sets, several measures, a clear description of what conditions we used, and an appropriate significance test with a low p-value.&lt;/p&gt;
&lt;p&gt;That way, when we say A is significantly better than B, we can be more sure that others will be able to replicate that. It’s not much. But it’s much better than guessing.&lt;/p&gt;
&lt;p&gt;The paper I am talking about here is this one. If you got interested, please see the references for a number of other good papers on the subject.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to be a Good Grad Student</title>
      <link>https://dirkhovy.com/post/2013_08_19_how_to_be_a_good_grad_student/</link>
      <pubDate>Mon, 19 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2013_08_19_how_to_be_a_good_grad_student/</guid>
      <description>&lt;p&gt;After almost 5 years, I am finally done with grad school! I just started a postdoc, so this is probably a good time to look back. Two friends recently asked me what my  “grad school story” was. What had I learned during my PhD, apart from the obvious technical and academic skills? What were the things I wish I’d known before I started. It got me thinking: what would I tell myself if I got to go back in time? Here is what I came up with:&lt;/p&gt;
&lt;p&gt;Take breaks&lt;/p&gt;
&lt;p&gt;When I started grad school, I believed exhaustion, all-nighters, and 14h-days were hallmarks of a good grad student. Well, turns out they are not! I worked 12-14h every day, 8h on weekends. After 3 months, I was tired all the time and got sick on a biweekly basis (flu, stomach bugs, etc.). After 6 months, I was burned out, deeply unhappy, constantly sick, and seriously considered quitting. Worst of all: my productivity had constantly decreased. It was time to rethink my believes.&lt;/p&gt;
&lt;p&gt;Grudgingly, I learned to accept those limitations. I concentrated productive work (coding, writing) in my peak hours, and used the rest for  “busy work” (reading, setting up experiments). Good time management is one of the key skills to learn in your PhD, and one of those they never teach you. It is simply impossible to produce quality work without taking a break every now and then. I have heard estimates that you can only be really productive for 6 hours each day.&lt;/p&gt;
&lt;p&gt;The most important part of that were the breaks! I exercised, walked, had lunch with friends, read the newspaper. I was still thinking about my work. But getting some distance from it helped me see errors I overlooked when working constantly. Cutting back on my hours not only made me happier and healthier, it also made me more productive.&lt;/p&gt;
&lt;p&gt;It also helped me to set myself an end time. I had some of my most productive times when I was dependent on a shuttle service and could only work until 5pm. I made every minute count, and went home in the evening without regrets. I got a lot done. Working 14 hours straight did not accomplish half as much―and felt a lot worse.&lt;/p&gt;
&lt;p&gt;Know your advisor&lt;/p&gt;
&lt;p&gt;When I started out in grad school, I thought of my advisor as this superhuman being who knows everything. Apparently, I am not alone. This honeymoon period can last a while. Inevitably, though, everyone reaches a point where they disagree with their advisor. It can be a bit of a shock to learn that advisors are only human, too.&lt;/p&gt;
&lt;p&gt;At the end of the day, though, it is good to remember that an advisor is the person who keeps you in business. They speak up for you in quals and screenings, vouch for you academically, introduce you to the right people, and help pay your tuition, travel expenses, and conference fees. They are busy people, and it is not their job to hold your hand, nor help you with the daily nitty-gritty. They have, however, spent a lot of time in the field and can help you find the right direction. It is up to you what you make of it, though. I know some people who have been discouraged by their advisor to pursue a project, only to find a paper on it at the next conference.&lt;/p&gt;
&lt;p&gt;It helps to know what their strengths are and benefit from them, and find somebody else for the things they cannot teach you. The latter are often hands-on solutions and technical issues. Most of their hands-on experience is several years old and might be outdated, especially in a fast-paced field like computer science. That’s what fellow grad students are for….&lt;/p&gt;
&lt;p&gt;It is your job to keep your advisor happy. Do the project work, help with classes, and listen to their counseling. But decide for yourself what applies to you. Part of your PhD is becoming your own researcher.&lt;/p&gt;
&lt;p&gt;Talk to people&lt;/p&gt;
&lt;p&gt;Even though it often felt like it, it was important for me to realize that I was not alone in the PhD! I was surrounded by other grad students and researchers, either directly or in my general community. When I started, I was lacking many of the computer science skills my peers had. I had the choice to either envy them or learn from them. The latter worked much better. I have learned more from water cooler talks and by asking colleagues than I have from most classes. I also learned a ton from collaborating on papers, and it’s less work for everyone. Internships and visits are a another great way to meet other people and get exposed to new ideas. I went to IBM and CMU, both for 3 months, and came back invigorated and full of new ideas and impressions.&lt;/p&gt;
&lt;p&gt;Learning how to talk to people also means giving good presentations. We need to share our ideas to get plenty of feedback. It helped me to find out how others perceived my research and to check whether that’s what I wanted to convey. If they didn’t get it, I reminded myself that it was probably my fault for not explaining it well enough: the audience is always right. This is especially true when talking to non-scientist friends: if I could explain it to them, I knew I had gotten to the core of the problem (this is sometimes also called the elevator pitch: can you explain your work to someone in the time you spent together on an elevator?). It’s difficult, but it helped me to think outside the box: there is always something in your work that relates to people’s everyday experience (even if it is remote). I think it also helps with writing papers―if you can explain what your work is about in a few simple sentences, people will be more willing to read your paper. Even scientists like a simple explanation better than a convoluted one.&lt;/p&gt;
&lt;p&gt;The more specialized my work got, the more important I found it to keep an open mind in general. I found that somebody who works on something completely different can often offer an objective view or an alternative approach to the problem. I made it a point to go to talks outside my area, read papers on related topics and general science. What others do can be as interesting as your own work (but don’t fall into the trap thinking what you do is less interesting than everybody else’s work). Also, it helped me overcome the misconception that being opinionated is equivalent to being smart. It’s an easy mistake to make, but it’s still wrong. And yes, I did it, and I’m not proud.&lt;/p&gt;
&lt;p&gt;Last but not least, keeping an open mind helped me to learn other things as well. Even though I felt challenged with all the demands of my research, I found that over time, my mind got used to challenges. It made it easier to pick up some new non-scientific skills along the way (I learned dancing, cooking, and how to ride a motorcycle), and I’m glad I did (again: it helps to take a break sometimes).&lt;/p&gt;
&lt;p&gt;It is impossible to be a good researcher when you never leave your room.&lt;/p&gt;
&lt;p&gt;Get a hammer&lt;/p&gt;
&lt;p&gt;In fact, get a whole toolbox! Early on, I was told to find an approach, algorithm, data set, resource, or other method that I liked. For me, this was the EM algorithm: I love how you can solve a circular problem (if I knew X, I could solve Y, and vice versa) by just starting out somewhere and then refining your model step by step. It’s similar to how children learn about the world, and it can help with a range of problems.&lt;/p&gt;
&lt;p&gt;Once I had that, I started looking for problems I could solve with it. I applied it to problems you cannot solve with it. That helped me understand why it works for some and not others. I learned a lot both about the problem and my hammer. It also expanded my technical expertise and helped me produce results more efficiently (and thus write more papers).&lt;/p&gt;
&lt;p&gt;It’s important not to get too hung up on one thing, though! Not everything is a nail, and nobody likes a zealous one-trick pony. While it sometimes seems that academia rewards single-mindedness, it often leads those people down a path of no return when the paradigm shifts or their technique becomes obsolete. I learned to accept the limitations and explored alternatives.&lt;/p&gt;
&lt;p&gt;I tried to put as many things in my toolbox as possible, and to learn when to use them. This is an immensely fun part of the PhD, and I don’t even think I’m done yet.&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;p&gt;This is probably the most important point. When I was so fed up with the program that I considered quitting, I paused and thought about why I put myself through this. Why did I do a PhD? And for whom? I realized that I was not in it for my advisor, for my family, or society as a whole, I wasn’t  doing this for others―I was doing this only for myself. Because it is what I always wanted to do! If I didn’t become the next superstar in my field, so what? I was in it because I loved it. Not every second of it, for sure, but as a whole: that was enough to make those difficult times pale to insignificance in the grand scheme of things. Around that time, I went to a talk by Tom Mitchell, on how to predict what people had read by looking at their brain images, and I remember walking out thinking  “There are so many more cool things I haven’t even started on, I can’t possibly quit now” (this is another reason why it is good to keep an open mind and check out other fields).&lt;/p&gt;
&lt;p&gt;When you’re in a PhD program, you are doing something very few people get the chance to do: you are at the cutting edge of research and work with interesting people on cool problems every day. Everybody gets down once in a while, and pretty much everybody considered quitting at some point. It’s good to remember what you’re excited about. And that you have every right to be excited!&lt;/p&gt;
&lt;p&gt;So that’s it. This is what got me through my PhD. If I had to do it all over again, this is what I would focus on.&lt;/p&gt;
&lt;p&gt;There are of course other good documents out there on how to make it through grad school, one of the better ones is this one by Hanna Wallach and Mark Dredze. Check it out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MACE available for download</title>
      <link>https://dirkhovy.com/post/2013_04_05_mace_available_for_download/</link>
      <pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2013_04_05_mace_available_for_download/</guid>
      <description>&lt;p&gt;Our software package MACE (Multi-Annotator Competence Estimation) is out! It provides competence estimates of the individual annotators and the most likely answer to each item. All you need to provide is a CSV file with one item per line.&lt;/p&gt;
&lt;p&gt;In tests, MACE’s trust estimates correlated highly with the annotators’ true competence, and it achieved accuracies of over 0.9 on several data sets. Additionally, MACE can take annotated control items into account, and provides thresholding to exclude low-confidence answers. Feel free to check it out. Comments welcome!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fake social network names won’t protect your privacy</title>
      <link>https://dirkhovy.com/post/2012_11_21_fake_social_network_names_wont_protect_your_privacy/</link>
      <pubDate>Wed, 21 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2012_11_21_fake_social_network_names_wont_protect_your_privacy/</guid>
      <description>&lt;p&gt;I have noticed that some of my friends (mostly Germans) use a fake name on social networking sites. This started a few years ago, when it became clear that a) the security of these sites isn’t exactly Fort Knox and b) their business model includes selling your data. I assume therefore that the fake names are meant to protect your private information. While I understand the sentiment, I think this is futile, and just makes it harder for your friends to find you. Here is why.&lt;/p&gt;
&lt;p&gt;The basic problem might be anthropomorphizing companies. If we assume  that social networking companies use the same approach to searching for our information as you and me, a false name could throw them off. (It would be tempting at this point to speculate about the age-old belief that knowing somebody’s name gives you power over them, but that’s beside the point here)&lt;/p&gt;
&lt;p&gt;However, these companies don’t use humans to search for your data―they use machine learning. And for that, a fake name is just one little piece of data. One of many…&lt;/p&gt;
&lt;p&gt;Say you just opened an account and put down a nickname. Can you ensure that all your friends will address you with that name, mention you with that name, and that you will sign all messages with it? Are all your stated relatives using the same moniker? If you ever found a long-lost friend on the site and wanted to contact, can you avoid to sent a message saying “Hey long-lost friend, this is really Dirk Hovy, I am using a nickname, but I would like to re-connect.”?&lt;/p&gt;
&lt;p&gt;If you answered “no” to any of these, all you managed is to make it a little harder, but not impossible, to get your real name. You more or less openly provided a decryption key that voids all your attempts at keeping your name safe. Just because you put something into a private message does not mean it is invisible. It’s just data, after all.&lt;/p&gt;
&lt;p&gt;Even if you managed to keep all of your communications under control: are you sure your account is not linked to any other sites that contain your name? Did you not sign up with this account when you bought something, have you not liked something with it, or linked it to some other account that contains your true name? If you have done any of the above, it will be the easiest thing in the world to find your true name and link it to your data. It is all a matter of connecting the dots. There are whole industries and research branches devoted to it, and the more dots there are, the easier it gets.&lt;/p&gt;
&lt;p&gt;I’m not trying to sound Orwellian, and I don’t mean to imply that those companies are evil by their nature. But their―more or less publicly―stated objective is that in exchange for letting you use their service, they get your data and sell it for profit. They are not in it for philanthropic reasons. They have bills to pay. You implicitly bought into that model when you signed up. You might have even explicitly agreed to it, provided you read all 25 pages of the end user agreement and were able to decipher the legalese. One can object to that model, but one cannot ignore the fact that it is reality.&lt;/p&gt;
&lt;p&gt;The most secure option is obviously to not use any social networking sites, or the internet, for that matter. While this is 100% safe, it is also not very realistic.&lt;/p&gt;
&lt;p&gt;So in the absence of that option, it is probably better to be more aware of what we put out there, and how easily it can be found. And if it is out there, it will be found and used. Don’t try to hide from a person if a machine is looking for you.&lt;/p&gt;
&lt;p&gt;Using a nickname just makes it hard for your friends to find you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trimming Papers</title>
      <link>https://dirkhovy.com/post/2012_09_25_trimming_papers/</link>
      <pubDate>Tue, 25 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2012_09_25_trimming_papers/</guid>
      <description>&lt;p&gt;Writing your papers in LaTeX is great and you should definitely do it. It makes everything better (with the possible exception of  grammar), but you have to trust it with the formatting. This is where it gets tricky. Most papers have a page limit, and while LaTex makes sure everything lines up perfectly, it does not care about how many pages it takes. Trimming the paper to a certain page limit thus becomes a familiar headache before every deadline. Luckily, you don’t have to rewrite the whole paper to make the limit. Here are some simple tricks I found helpful to save a lot of space.&lt;/p&gt;
&lt;p&gt;Don Metzler showed me a great and easy technique to trim your paper considerably:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;find all paragraphs that have three or fewer words on the last line&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;shorten those paragraphs so that the last words advance into the previous line&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can leave paragraphs with more than three words on the last line alone, so instead of rewriting everything a bit, you can focus on a few paragraphs.&lt;/p&gt;
&lt;p&gt;This only eliminates one line per paragraph, but due to the way LaTex spaces out paragraphs over the page, this actually shortens the overall paper quite a bit. Treat three or four paragraphs that way and you might cut your paper by half a page.&lt;/p&gt;
&lt;p&gt;So, how do you shorten those paragraphs? A good way is to get rid of redundant or  “empty” expressions. One that I found myself using way too often is  “especially in the case of”, as in  “This is annoying, especially in the case of long paragraphs”. The expression is perfectly grammatical, but we can convey the same meaning by just using  “especially for”, as in  “This is annoying, especially for long paragraphs”.  “Especially” already singles out a special case, so we don’t have to say it again. We don’t lose any information, but save three words, and―what’s more important in LaTeX―three white spaces. LaTex spaces out words evenly across each line, mainly by varying the size of spaces (it also varies character spacing, but to a lesser degree). So having fewer characters and white spaces shortens the line, which in turn shortens the paragraph, which in turn shortens the page, which in turn allows you to keep your page limit.&lt;/p&gt;
&lt;p&gt;Other phrases that can be shortened: verb plus nominalization, if there is a proper verb for it. I find myself using lots of these constructions. Instead of saying  “we used this for our evaluation”, just make it  “we evaluated this”.&lt;/p&gt;
&lt;p&gt;The Chicago manual of Style also identifies these candidates:&lt;/p&gt;
&lt;p&gt;“due to the fact that” =  “because”&lt;/p&gt;
&lt;p&gt;“in connection with” =  “of”,  “about”, or  “for”&lt;/p&gt;
&lt;p&gt;“at this (point in) time” =  “now”&lt;/p&gt;
&lt;p&gt;The best way to save space, however, is to delete useless phrases. Many papers include a paragraph which starts with  “The remainder of this paper is structured as follows:…”. I automatically skip ahead if I see this. In a 8-page paper, you do not need an overview: I can get that by just flipping through. After all, that’s what section titles are for. And do sentences like  “We first introduce the problem in Section 1” or  “Section 5 concludes the paper” really add anything to my understanding? Do they need to tell me that the section titled  “Evaluation” will  “present the evaluation of the experiments”?&lt;/p&gt;
&lt;p&gt;Leaving this overview-paragraph out saves a lot of space, and does not take anything away from the content.&lt;/p&gt;
&lt;p&gt;Of course it’s good to pay attention to these things while writing, and express yourself as clearly and succinctly as possible. But a few of these cases will creep in anyways. And when it is time to trim the paper, they are a good starting point.&lt;/p&gt;
&lt;p&gt;If you have more tips or suggestions, please share! Let’s make meeting page limits in LaTex less scary.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Art of Good Presentations</title>
      <link>https://dirkhovy.com/post/2012_09_14_the_art_of_good_presentations/</link>
      <pubDate>Fri, 14 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2012_09_14_the_art_of_good_presentations/</guid>
      <description>&lt;p&gt;I have talked before about how important it is for scientists to express themselves well, and the most important aspect of that is to give good talks. I am far from being a good speaker, but I am a little obsessed with learning what makes one.&lt;/p&gt;
&lt;p&gt;So I recently went to a workshop on presentation, and came away with some good tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;use dark background. It is much easier on the eyes of your audience, broadens your screen estate, and prevents you from casting weird shadows when you stand in front of it (some people dislike it, though, because it’s so dark)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;shape your talk like a glass: start broad and then narrow to the details (the cup of the glass), stay on them for a while (the stem), and end broadly (the foot)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;maximize the axis space of graphs to fill as much of the screen as possible. Push the legend and title into the graph area, in blank spaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;do not use a laser pointer. If you want to point something out, circle it on the slide&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the best ways to get better is to watch good presentations and note what they do. Here are a few presentations I particularly enjoyed, and what I think makes them interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dick C. Hardt on  “Identity 2.0”. I have no idea what  “Identity 2.0” is, and I don’t think it caught on, but the rapid-fire presentation style is fascinating and easy to prepare. Though hard to pull off…&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guy Kawasaki’s talk for entrepreneurs uses minimal slides, and a lot of great lines. Some of what he says is even relevant for presentations, but mostly, it is fun to watch and easy to follow.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chip Kidd talks about book covers, but he drives home an important point: show or tell, but not both―your audience is not stupid.  “And they deserve better.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The previous talks are about big ideas, and thus a bit abstract. Hans Rosling shows how you can take hard data and make its presentation palatable and fun. This takes a lot of work in preparation, but it shows you that you don’t always need the same old boring graphs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Similarly, David McCandless shows how information can be conveyed in interesting and appealing ways. Maybe not always achievable for the average scientist, but worth thinking about, and looking at.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What comes through for me in all the good talks is this: keep it simple. Use pictures more than words. Your slides should be secondary to your talk. They do not need to be interpretable without you. That’s what a paper or a handout is for.&lt;/p&gt;
&lt;p&gt;I recently tried cutting text as much as possible in my proposal talk, and got very positive feedback about the slides. It is harder for scientific presentations than for general talks, since you want to convey a lot of detail and nuances, but it helps to focus the attention. I plan to reduce to the max.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orange Chicken</title>
      <link>https://dirkhovy.com/post/2012_04_15_orange_chicken/</link>
      <pubDate>Sun, 15 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2012_04_15_orange_chicken/</guid>
      <description>&lt;p&gt;Together with my roommate, I found a great way to prepare chicken. It cooks quickly, stays moist, and is almost impossible to mess up. I made variations of it four times during the last week. Here goes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Take chicken breast, pat dry and cut into small strips.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Put strips in a ziplock bag and add salt and a few table spoons of corn or tapioca starch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Heat a pan, add oil.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shake chicken strip in a strainer to get rid of excess starch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fry the chicken in pan.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can stop here and eat the delicious, juicy chicken. The starch creates a thin layer of insulation between meat and pan, so that the chicken cooks more evenly and doesn’t dry out. You can add other spices to the starch, if you are so inclined. Allspice is pretty awesome.&lt;/p&gt;
&lt;p&gt;Or you can go in to make an orange chicken that beats the crap out of anything you get at a Chinese fast-food restaurant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mix the juice from 3 limes with the same amount of orange juice and some fermented chili sauce.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add to the cooked chicken strips.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reduce until strips are just coated with a thin film.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>In Other Words</title>
      <link>https://dirkhovy.com/post/2012_03_29_in_other_words/</link>
      <pubDate>Thu, 29 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2012_03_29_in_other_words/</guid>
      <description>&lt;p&gt;While writing on my thesis and various papers, I found that there sometimes is a disconnect between my perception and what others make of the same data. I started thinking about why that is and how it could be solved. I found a simple, yet effective solution: have other people tell you their version of the story. Here is why.&lt;/p&gt;
&lt;p&gt;One of the most important aspects of research is communicating your ideas. It does not help the world if you are brilliant but cannot convey your thoughts. It is also one of the most difficult tasks. What you want and what your average reader wants differs slightly, and while you know your needs, in the end, it is the needs of your readers you have to cover in order to convey your idea.&lt;/p&gt;
&lt;p&gt;By the time you are ready to publish, you have spent a lot of time setting up experiments, tweaking parameters, searching related work, and collecting data points. You have devoted a sizeable part of your life to this, you know all the details, and you are very attached to the outcome. You want the world to know how much work it was, and to be able to understand it in all its complexity.&lt;/p&gt;
&lt;p&gt;The awful truth is: most people do not care about the details of how you reached your final results. At all. They want a take-home message they can readily understand themselves and relate to others. And they should get one!&lt;/p&gt;
&lt;p&gt;Dwelling on the details might make your paper very reproducible, but it is also a surefire way to drive away your readers. They will soon lose interest and skip the details, trying to find what they are looking for. Or stop reading altoghether. If this happens, all your work was basically in vain. They won’t get your idea, and they won’t tell others about it.&lt;/p&gt;
&lt;p&gt;So how can you meet your readers needs?&lt;/p&gt;
&lt;p&gt;A solution that worked surprisingly well for me was to simply ask them. Tell your friends/colleagues the general problem, give them a few data points, and ask them what they think the paper looks like (obviously, don’t give them your version yet). You’d be surprised how much the stories can differ.&lt;/p&gt;
&lt;p&gt;Your friends are unburdened by the details, and still able to see the forest instead of the trees. If they ask you for more information, supply it. You will learn which parts only you saw (because you spent so much time on it), and you can go back and make them clear(er).&lt;/p&gt;
&lt;p&gt;Pay attention to how they would present your findings. What do they emphasize, what do they leave out, what is the story they spin? If they reach another conclusion, maybe you need to give them more information, or you have to re-evaluate yours. Don’t reject their outline thinking they did not understand it. If they don’t, neither will your reviewers!&lt;/p&gt;
&lt;p&gt;If you do it with enough people, you will find things that pop up again and again, and the holes that need to be filled.&lt;/p&gt;
&lt;p&gt;This solution is obviously not foolproof. You have to be able to let go of some parts you really liked, and you have to be able to draw attention to some important your helpers might have skipped. It can not guarantee you an accepted paper, but it will help you to make it more readable, and convey your idea better. Also, it’s a good way to let your friends know what you’re working on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Science and Showmanship</title>
      <link>https://dirkhovy.com/post/2011_10_16_science_and_showmanship/</link>
      <pubDate>Sun, 16 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2011_10_16_science_and_showmanship/</guid>
      <description>&lt;p&gt;German researchers have drafted a position paper in which they demand science be decelerated in order to improve its quality. Their points are (Die Zeit article 4/14/2011):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;worldwide reduction of publications to allow scientists to survey the field and ensure quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;research needs a basic funding, yet cannot be economically evaluated like a company&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;funding should be based on content, not projected success&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;authors should only appear on papers if they contributed to it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scientists have to write their own grant proposals, no agencies should do that or even correct the scientists&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;experiments need to be more transparent and reproducible&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;good science is only possible with long-term grants&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I agree with most of the ideas (I do think that a base funding would be a Very Good Thing for a couple of less flashy disciplines, and I do agree that science should be about substance first), I take issue with the latent notion that science is too fast, too competitive, and that presentation is overrated.&lt;/p&gt;
&lt;p&gt;Science is all about ideas, even half-baked ideas, and, more importantly, sharing them. No major work was created by one person out of thin air, but resulted from building on what other people have done before, however small it was. If those other people had waited to publish it until they thought it was complete, it might have never seen the light of day. Or, more likely, it would have, but published by somebody else, who was not as hesitant. Of course you should wait until you are reasonably sure your results are sound, but there is a point where it turns into procrastination. If you do not publish, nobody knows you are brilliant (they also won’t know if you are clueless…)&lt;/p&gt;
&lt;p&gt;Part of a scientist’s skill set is to navigate and assess the body of work in his or her field. There are increasingly more tools to help you achieve that. Scientists know which journals are hard to get into, and which ones will print anything as long as it has a title. Researchers will assess work also based on where it is published. Both quality and quantity matter. Someone who has had only one paper in 10 years, but in Nature or Science, is not much better than someone who has cranked out four papers a year in obscure journals over the same time span. Granted, the first guy has substance, but who tells me he could do it again? With the other one I know at least what he was up to, and that his ideas were bad. Luckily, most people will lie somewhere in the midlle. So the flood of publications is actually a boon rather than a bane.&lt;/p&gt;
&lt;p&gt;By artificially restricting the number of publications, you do not necessarily improve quality (transparency and fairness of acceptance criteria is an issue to itself), but take away a lot of breadth and information.&lt;/p&gt;
&lt;p&gt;And, yes, science is about presentation: if your idea is too complicated to explain it, chances are it is not worth explaining anyways. Some people maintain that you should be able to explain your whole research idea during an elevator ride. A lot of the great ideas are exceedingly simple, and a lot of good papers are good because they explain their point well. A brilliant mind that cannot communicate its brilliance is no use to the academic world, least themselves. The fact that the occasional showman gets a grant although his ideas are not very deep should not stop us from rewarding good presentations!&lt;/p&gt;
&lt;p&gt;You might not like it, but I am sure that fast, competitive, and presentable science improves our general knowledge and understanding of the world. Artificial boundaries and regulations do not. The times when researchers could sit in their study and worry about one thing for years are gone. Now, you have to go out and present it, for money, for visibility, and ultimately also for the advancement of science.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>One wish</title>
      <link>https://dirkhovy.com/post/2011_04_26_one_wish/</link>
      <pubDate>Tue, 26 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2011_04_26_one_wish/</guid>
      <description>&lt;p&gt;One thing I wish I was, apart from brilliant, is to be fascinated by boring things.&lt;/p&gt;
&lt;p&gt;Think about it: it would have so many advantages. Like that linear algebra class you had in high school when you could barely stay awake, and now you try to remember how to invert a matrix. Or the list of all the resources that everybody on you project agrees would be really useful to have, but nobody wants to actually sit down and compile it, because the thought alone makes half your brain fall asleep.&lt;/p&gt;
&lt;p&gt;If you were excited by all of that, you could get a lot of good work done. On the downside, you might also become the go-to guy for everyone with a boring task. Hey, you can’t have everything! At least you wouldn’t be bored…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Language change</title>
      <link>https://dirkhovy.com/post/2011_04_15_language_change/</link>
      <pubDate>Fri, 15 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2011_04_15_language_change/</guid>
      <description>&lt;p&gt;After some deliberation, I decided to write my future posts in English only, to speed up my blogging freqeuency.&lt;/p&gt;
&lt;p&gt;Translation took up too much time, and in some cases prevented me from posting at all. I will catch up on these posts now.&lt;/p&gt;
&lt;p&gt;Since my German readers have excellent English skills, this solution leaves nobody out.&lt;/p&gt;
&lt;p&gt;You can expect more posts in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Remembering the Dead, 1</title>
      <link>https://dirkhovy.com/post/2010_11_20_remembering_the_dead_/</link>
      <pubDate>Sat, 20 Nov 2010 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2010_11_20_remembering_the_dead_/</guid>
      <description>&lt;p&gt;My grandmother could never throw anything away. Occasionally, my mom and her sisters would clean out the pantry, and my cousins, siblings, and I would stand by and bet on the oldest item. A ten year old ketchup bottle? A pack of custard powder several years over its due date? Or maybe something that had to be bought with post-war food stamps?&lt;/p&gt;
&lt;p&gt;My Grandmother had a very different approach to food than we do. She raised six kids on a budget, during and after the war. If something grew moldy, she would cut out the soiled parts and declare the rest perfectly edible, and, in fact, eat it. I don’t think she ever got sick. She told us that dirt made you healthy, and my mother related to me that as kids they believed one pound was the acceptable amount of dirt per year.&lt;/p&gt;
&lt;p&gt;My grandmother had learned cooking professionally, and when I say professionally, I mean efficient, not fancy. For our family dinners, she cooked for an army, and her meatloafs and patties are legendary. My mom’s were good, but these were heavenly, probably because my grandma believed that more fat makes anything better. She was a round woman with rosy cheeks, and in my memory, she always wore a flowery dress and a grey wig. She would roll down hills with us kids and could pop out her dentals, which we thought was the coolest thing ever. If we were grumpy, she would give us  “laughing pills”, what other people called Mentos.&lt;/p&gt;
&lt;p&gt;Her two worst memories were the night they bombed the neighboring city, when the horses screamed in their stables ( “Have you ever heard horses scream”, she would whisper with a shudder), and the Polish soldier who stole her strawberries. When she grew old, she was cared for by a Polish nurse, and I think Poland was able to redeem itself in her opinion. After the death of my grandfather, she got very sad and only wanted to be reunited with him.&lt;/p&gt;
&lt;p&gt;She died during the night, with one eye open and a surprised look on her face.&lt;/p&gt;
&lt;p&gt;I often think of her when I cook, and what she would have done. I collect and filter the grease whenever I fry bacon. I buy cheap cuts of meat, like shoulder or chicken hearts. I think fat makes everything better. When I cook, I cook for an army.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New York I love you, but you’re bringing me down</title>
      <link>https://dirkhovy.com/post/2009_12_15_new_york_i_love_you_but_youre_bringing_me_down/</link>
      <pubDate>Tue, 15 Dec 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_12_15_new_york_i_love_you_but_youre_bringing_me_down/</guid>
      <description>&lt;p&gt;Like millions before me, I arrived in New York with a sense of wonder. I had been reading on the train ride into Penn station, so my first glance of the city was when I stepped out onto seventh street. It was a clear midwinter afternoon, and the low sun illuminated the tips of the skyscrapers and filled the streets with a soft light. I was immediately captivated. I had been meaning to come here for a long time, and finally I had made it.&lt;/p&gt;
&lt;p&gt;Unless millions before me, however, I had not come from distant shores to build a living here. I was only visiting for an afternoon from New Jersey.&lt;/p&gt;
&lt;p&gt;My first action was to find a Starbucks, something I imagined to be a little easier in New York. Eventually, I succeeded, got a coffee, and left through the backdoor into some sort of mall. Uniformed pages showed people around, a group of women was taking pictures of the ceiling. I started to wonder…&lt;/p&gt;
&lt;p&gt;Only when stepping out onto fifth street and glancing up the facade, I realized that I had just unknowingly visited the Empire State Building. They should put up signs…&lt;/p&gt;
&lt;p&gt;New York is very different from LA. There are no Empire State Buildings, for a start. Also, new York is reigned by pedestrians. As soon as the cars slow down, they start crossing the street, no matter whether it’s red. If you do that in LA, you’ll get fined for jaywalking. Here, you only get fined for not walking.&lt;/p&gt;
&lt;p&gt;Nobody asked how I was doing or wished me a good day, and I adapted quickly to the environment. I rolled my eyes at people slowing down, I snarled at people stopping to watch the shop windows. I’m sure New Yorkers have their bad reputation because of visiting Angelinos who enjoy being rude for a day.&lt;/p&gt;
&lt;p&gt;Ok, not true, I did neither of the above. Without looking at anyone, I just walked at a quick pace up the street, headed for Central Park before it gets dark. I saw Macy’s windows and the Rockefeller Center christmas tree. I stopped at a cathedral that stood in stark and comical contrast to the high risers around it, seeking some rest from the bustling streets, but it was just as busy inside as outside. I strolled through the beginning of Central Park and got a bad Espresso and hot chocolate at a hip cafe. Eventually, I swam anonymously through the crowds down Broadway, washed into Penn station, and boarded the next train back to New Jersey. A state that is much nicer than it’s bad reputation. I’m almost sure it’s due to visiting Angelinos…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Belated Birthday Wishes</title>
      <link>https://dirkhovy.com/post/2009_12_02_belated_birthday_wishes/</link>
      <pubDate>Wed, 02 Dec 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_12_02_belated_birthday_wishes/</guid>
      <description>&lt;p&gt;Sometimes, you see things only from a distance. Birthdays, for example (whoever had their birthday lately knows that I need a little longer). When the Wall fell, I was eight. At that time it seemed nothing special. I only remember that everyone was very excited and watched a lot of TV. This was rare (the watching television), and therefore had to mark something special. I was told that there had been a border in Germany, which was now gone. That did not impress me much. When we went on holiday in France, there was a border, too, and that was always a lot of fun. Also the fact that they spoke German on the other side of the border I found little remarkable. I had an aunt in Austria, and there was a frontier, too, and on the other side they spoke German. With eight, geopolitics is still rather simple…&lt;/p&gt;
&lt;p&gt;Only when I look back now, things seem more remarkable. And more complex. Germany and Europe are what they are, last but not least because of those days in the fall, when my family watched a lot of TV, and everything that ensued. And not, as some here would have you believe, because a senile ex-actor proclaimed  “Tear down that wall” (even less because of a third rank star with fake chest hair humming silly little song about freedom. But then, nobody but him believes that anyways). And it is good the way it is. At least, from a distance, it does not look half as bad as one would have it at home.&lt;/p&gt;
&lt;p&gt;Sometimes you only miss things from a distance. Things that you have not previously noticed, or found ridiculous. Many little things: long train rides through wooded hills, deli-meat-specialist saleswomen, autumn fires, bakeries, Feierabend beer, shop talk about football, the deep rooted belief that everything in this world can be solved efficiently in a very specific way.&lt;/p&gt;
&lt;p&gt;But of course, mainly the loved ones you left behind, in that reunified country on the other side of the globe.&lt;/p&gt;
&lt;p&gt;And before you know it, you find youself sentimentally murmuring what Hoffmann von Fallersleben wrote down nearly 170 years, Einigkeit und Recht und Freiheit… Happy Birthday, Germany! Be well, and stay as you are. You are ok the way you are.&lt;/p&gt;
&lt;p&gt;Like I said, I always need a little longer for birthdays…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Food Nerd</title>
      <link>https://dirkhovy.com/post/2009_09_24_food_nerd/</link>
      <pubDate>Thu, 24 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_09_24_food_nerd/</guid>
      <description>&lt;p&gt;For some time now, I have been a subscriber to the magazine  “Cooks Illustrated”.  A friend described it as  “food porn”, but the focus is less on sensual pleasure than on scientific analysis (you know that a food magazine is serious if they print in two columns, and only in B/W).&lt;/p&gt;
&lt;p&gt;Recipes are varied ingredient by ingredient, tested and the outcome reported, until you get the best possible result. In addition, you get tricks and tips and kitchen tool reviews.&lt;/p&gt;
&lt;p&gt;My absolute favorite by now is a recipe for ricotta. Super easy, fast, and very delicious! And a worthy replacement for the unobtainable Quark. Here’s my  version (original in Cooks ilustrated Oct 2009):&lt;/p&gt;
&lt;p&gt;Heat a gallon of whole milk with a tablespoon salt to 185°F, or until surface slightly ripples. Take off the flame and add 1/3rd cup of lemon juice. Let stand for 5 minutes. If the consistency is not curdly enough, add another tablespoon of lemon juice. Repeat until there are no more changes in the consistency. Skim off the mass with a strainer and put it in a colander lined with kitchen towel. Put over a bowl and leave overnight in the fridge.&lt;/p&gt;
&lt;p&gt;The next morning, you have fresh ricotta, somewhere between cream and cottage cheese. Tested in pasta and with honey for dessert.&lt;/p&gt;
&lt;p&gt;Guten Appetit!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summertime</title>
      <link>https://dirkhovy.com/post/2009_09_06_summertime/</link>
      <pubDate>Sun, 06 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_09_06_summertime/</guid>
      <description>&lt;p&gt;And then, suddenly, it is summer. The days are only a little bit hotter, yet the nights are warm and mediterranean. If Angelenos sat outside, this is when they’d do it. Yet people are fleeing LA for the long weekend, clearing the freeways, leaving the city behind.&lt;/p&gt;
&lt;p&gt;Fires rage on the hills encircling it. At night, you can see their orange glow on the slopes. By day, an unwavering pillar of smoke marks their position. It mingles with the smog of downtown and tints the sunsets pink and orange. It rains soot over the city and can be smelled as far as the coast. And it greets the people coming back into LA as they fly through it: You might leave for a weekend, but the city is still here. With its fires.&lt;/p&gt;
&lt;p&gt;And so is summer…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Science vs Engineering</title>
      <link>https://dirkhovy.com/post/2009_08_20_science_vs_engineering/</link>
      <pubDate>Thu, 20 Aug 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_08_20_science_vs_engineering/</guid>
      <description>&lt;p&gt;There are two kinds of researchers: scientists and engineers. Faced with a problem, the scientist will say  “How interesting” and proceed to abstract and classify it, develop experiments to reproduce it, and come up with a theory to understand it.&lt;/p&gt;
&lt;p&gt;Faced with a problem, the engineer will say  “How can we solve this” and proceed to measure and discretize it, build a model, refine and rework it, until it solves the problem.&lt;/p&gt;
&lt;p&gt;Thanks to science, our understanding of the problem has increased. Thanks to engineering, we have one problem less. Ideally, these two disciplines should work hand in hand. Scientists analyze the problem to help understanding it, with an eye on possible solutions. Then engineers use that knowledge to solve the problem more efficiently. And indeed, scientists often try to sound more engineering, and engineers more sciency. But that is mostly wishful thinking.&lt;/p&gt;
&lt;p&gt;In reality, the two camps know little and think even less of one another. Scientists easily get absorbed and sidetracked by fascinating details, producing knowledge for the sake of knowing. Engineers get just as fascinated, yet with task specific details, tailoring their solutions so exactly to the problem at hand that they have to start almost from scratch when faced with a similar one.&lt;/p&gt;
&lt;p&gt;“What do I care why it works, as long as it does”, says the engineer. And more often than not, it involves some hack to get there.&lt;/p&gt;
&lt;p&gt;“What do I care how it works, as long as we learn something”, says the scientist. Yet sometimes, even that is not guaranteed. The main difference between the two is the nature of the outcome. Engineering sells better, science―not so much.&lt;/p&gt;
&lt;p&gt;Linguistics is clearly a science (I could not think of any product linguistics has fostered). Computer science, despite the name, is mostly engineering (unless it’s theoretical CS). Somewhere in the middle, there’s Computational Linguistics, and there, waving, is little me…&lt;/p&gt;
&lt;p&gt;In this ambivalent field, one seems to have to choose a side. Being a linguist by training and nature, I am primed on sciences. Yet getting a PhD in CS often incurs being asked to solve engineering problems which have a linguistic component. It’s like trying to make me an engineer with a knack for language. It took me the better half of a year to realize that that is not who I am: I want to us the toolkit of CS to understand linguistic problems. A scientist with a knack for engineering, maybe. A fine distinction, yet an important one. At least for the person who makes it… But what does it matter, as long as we can bridge the gap between the two!&lt;/p&gt;
&lt;p&gt;Thanks to engineering, this is the first post I wrote in English and then translated back into German. And thanks to science, I was able to know which parts I should correct. And why…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bratwurst mit Sauerkraut</title>
      <link>https://dirkhovy.com/post/2009_08_12_bratwurst_mit_sauerkraut/</link>
      <pubDate>Wed, 12 Aug 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_08_12_bratwurst_mit_sauerkraut/</guid>
      <description>&lt;p&gt;I love eating! And I love cooking. Perhaps even more than eating. Whoever is surprised by this does not know me well. Cooking for me is far more exciting than eating the final product. Perhaps because it has no more secrets. The interesting part is to figure out if everything went smoothly, as I had hoped for. And rather than for me alone, I cook for others―the more the better. Eating is the most direct way to show someone you like them, and I haven’t yet met anyone who does not appreciate that.&lt;/p&gt;
&lt;p&gt;Food is―besides language―one of the most salient features of a culture. But more often than not, the most famous dish is not everyday food. When it comes to national cuisine, we all go back to stereotypes: as a German, of course, I love beer, miss bread with crust, and would die for bratwurst with sauerkraut.&lt;/p&gt;
&lt;p&gt;Darkly, people have prophesied before my arrival in LA that I will be a wabbleing lardass in no time, thanks to a diet of hamburgers, French fries and donuts. What else do Americans eat…?&lt;/p&gt;
&lt;p&gt;But wait! Americans love good food. They celebrate it! There is a TV station which broadcasts nothing else, there are countless journals, websites and local groups, sharing insider tips, recipes and restaurants. And California is especially ideal for this. You get everything fresh: meat, fish, vegetables, fruit―the producers are only a few minutes drive away. Accordingly, there are Farmer’s Markets in each district, and the supermarkets carry everything your heart desires. And at any time.&lt;/p&gt;
&lt;p&gt;Each immigrant group has brought their own recipes, adjusted them to the local produce, and all peek into their neighbor’s pots. Americans are new to the international market of cooking traditions, but they have no inhibitions, they’ll try everything. And without the ballast of tradition, they pick from each recipe the best and continue from there on.&lt;/p&gt;
&lt;p&gt;Californian wine can easily keep up with Bordeauxs, but costs only half as much. And, it has to be said: American beer is excellent! I have  found a lot of great beers, often by small local breweries, which are not exported. What you get in Germany is waht nobody here drinks. (And no, German beer is not automatically the best in the world. Purity law or not: some insipid brews I have chugged out of patriotism were no advertisement for the German brewing tradition).&lt;/p&gt;
&lt;p&gt;Downtown, there is restaurant with the beautiful name  “Wurstküche”, and from Knack to Bratwurst and rattlesnake-rabbit links, they serve everything you can cram into guts.&lt;/p&gt;
&lt;p&gt;Only the bread here still needs practice. No crust and fuzzy consistency―this can pass as “bread-like pastry” at the most. I’m doing my part to educate and provide home-baked goodies for my colleagues. Multigrain with spices, coriander with apricot and pistachio, or chocolate with cranberries―I have adjusted my palette to the local palate, spread the recipes and wait for it to bear fruit. The reactions are unanimously positive. The only setback: The crust was too cross, I was told. But there I am not willing to make compromises! As I said, we still practice …&lt;/p&gt;
&lt;p&gt;The problem with German stereotypes, actually, is that although they are not representative, they all apply to me… Bread, sausage and beer? Bring it on!&lt;/p&gt;
&lt;p&gt;My eaten Donuts, on the other hand, I can count on a few fingers, for the Hamburgers I need two hands. In fact, I have lost 10lbs within 3 months, simply by eating more consciously (and a little stress). Even in America there is salad…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Movie World</title>
      <link>https://dirkhovy.com/post/2009_06_29_movie_world/</link>
      <pubDate>Mon, 29 Jun 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_06_29_movie_world/</guid>
      <description>&lt;p&gt;Apparently, I just live down the road from the bar that was the inspiration for Moe’s tavern in the Simpsons. I will check that out…&lt;/p&gt;
&lt;p&gt;Can I talk to Mr. Freely? First name I.P…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Insights of a travelling salesman</title>
      <link>https://dirkhovy.com/post/2009_04_28_insights_of_a_travelling_salesman/</link>
      <pubDate>Tue, 28 Apr 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_04_28_insights_of_a_travelling_salesman/</guid>
      <description>&lt;p&gt;When I arrived here, my attitude was the same as every European’s fresh off the boat―a feeling of calmly assumed cultural superiority. The same kind of feeling you have towards high school kids talking about poetry, or the new guy in your office droning on about workflow. Experience, time, let’s face it: History is on your side! Surely, coming from a continent that has such a diverse culture, such a cornucopia of wars, famines, great thinkers and glorious artists makes you a more sophisticated human being than these youngsters? I laughed at  “historical buildings” that are barely 200 years old, I chuckled at the subject of American history.&lt;/p&gt;
&lt;p&gt;But working in America is a humbling experience. People get up early and go to bed late. There is no sentimentality lost, work has to be done, no matter what. The only thing you are judged by is the impact of your work. And you realize: All your sophistication, culture and history buy you nothing! The people who came here more often than not did so because they wanted to leave their old lives behind. Together with the history, the culture, and the prejudices. Coming here was making a clean slate―your religious beliefs, your philosphical views were secondary to your ability to make a life. History was what you made for yourself. You can knowledgeably talk about medieval poetry, Romanticism and dialectic? Good for you! Now, concerning that deadline…&lt;/p&gt;
&lt;p&gt;And then, what kind of history would a German and a Chinese immigrant have to share? The first point in time they both could relate to was the time they arrived here. What each of them had thought of as historical facts was just an interesting story from another place and time to the other. Even if you did not want to leave the past behind, it was something you shared with a much smaller group, something private and reserved for special occasions. So while many Americans treasure their heritage and take interest in the countries of their ancestors, they do so in their spare time. History is something that happened in the past, but we are living now!&lt;/p&gt;
&lt;p&gt;And just in case it becomes history some day, we better do a good job in the meantime…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Picture this…</title>
      <link>https://dirkhovy.com/post/2009_04_12_picture_this/</link>
      <pubDate>Sun, 12 Apr 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_04_12_picture_this/</guid>
      <description>&lt;p&gt;Picture the late morning sun over LA, lazily shining through the open front door of a single storey house. As it crosses the threshold, it passes waves of Tango music, floating out into the Easter Sunday.  As it hits the battered hardwood floor, legs move swiftly through the beam, wearing high heels and dancing shoes. On a table in the kitchen wait banana pancakes, tamales, fruit, and orange juice for the dancers.&lt;/p&gt;
&lt;p&gt;What a perfect way of celebrating Easter…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>After rain comes sunshine</title>
      <link>https://dirkhovy.com/post/2009_03_02_after_rain_comes_sunshine/</link>
      <pubDate>Mon, 02 Mar 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_03_02_after_rain_comes_sunshine/</guid>
      <description>&lt;p&gt;“You write so infrequently”, I hear quite often.  “I have too much to do,” I then reply. That is true, but there is of course  more. Everyone likes to hear good news, or at least exciting ones. And in recent weeks, there was not much of either of them. Not only the global economy, also my life showed signs of unhappy development. And the Californian sun was behind thick clouds.&lt;/p&gt;
&lt;p&gt;Back from Germany, I got really aware of how far away I am. Not only geographically, also mentally. My attempt to tell people here about Germany showed me how much I’ve taken for granted, how little things I questioned or had consciously perceived. How does the German insurance system work, what exactly does the Bundespräsident do, why  are there Haupt and Real schools (and how do they translate), and where is the difference between the Bundesrat and Bundestag?&lt;/p&gt;
&lt;p&gt;At the same time I am still a foreigner here: I know no American lullabies, was spared from high school hell, and so far I have never thought about my credit report. I felt like sitting between all chairs. I even thought my English was deteriorating.&lt;/p&gt;
&lt;p&gt;In addition, my research inched forward slowly, I long puzzled over my schedule, and there were some things in my private life I had to set straight. Nothing great, but in sum unnerving. My morale was struck. To make matters worse, in February, I had an accident. No physical consequences, but the car was out, and that in LA is synonymous with disaster. The low point was reached.&lt;/p&gt;
&lt;p&gt;Perhaps a good thing, because from there on it went uphill. After a shock, you can  see things more clearly: I have reorganized my week, redefined my targets, and  concentrated on fewer things, but with more energy. And that did it. I am happier and more content than before, university and job are fun again, and the thing with the car was also taken care of. The insurance was helpful and friendly, many people have given me advice and practical help, and since yesterday I’m glad the owner of a bright red Nissan Versa. I am still a foreigner, but I am not the only one. And it can be quite charming, too. Just what the deal with Hauptschulen is escapes me still…&lt;/p&gt;
&lt;p&gt;So, this is it! After every rain there’s sunshine, even and especially in California…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Have a nice Vorurteil!</title>
      <link>https://dirkhovy.com/post/2009_01_31_have_a_nice_vorurteil/</link>
      <pubDate>Sat, 31 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_01_31_have_a_nice_vorurteil/</guid>
      <description>&lt;p&gt;One of the most frequently encountered prejudices against America in Germany is the superficiality: In America, nobody is really friendly, that is just superficial. Service in restaurants lasts only until the check is brought. Admittedly, the greetings in America are much more cordial than in Germany, and still the phrases are less comitting. Nobody is really interested in  “how I am” (I started relating at length how I felt once and was met with blank stares). Is that necessarily worse, though? I had to un-train myself wishing random people a nice day when I was in Germany, just to avoid being eyed with suspicion.&lt;/p&gt;
&lt;p&gt;I don’t really know what people in Germany expect: If I meet someone in an elevator, I don’t want to share their most intimate thoughts, a  “How is it going” is sufficient. It does not hurt to say something friendly, and it is always nice to hear it. If I enter a restaurant, I do not intend to malke friends for life, I just want to be served promptly and correctly. Maybe with a smile, why not?&lt;/p&gt;
&lt;p&gt;And though people look down on the American attitude, nobody is really fond of the  “emotionally authentic” German service. Maybe waiters there are more authentic, yet if I have to wait 20min for some sourpuss to bring me the espresso I ordered twice, which then consistently does not turn up on the check, I have to say that I don’t give a damn about emotional depth and authenticity! After all, German waiters do not even have to worry about the expected tip being subtracted from (already minimum) wages, so it should be much easier for them to smile some times. The friendliness of American service personel might be partially due to the fear of losing one’s job, but what difference does the motivation for friendly service ultimately make for the customer? I do not need to questions someones psychological motivation if he treats me nicely. And just for the sake of completeness it should be mentioned here that Americans are indeed capable of genuine friendliness and helpfulness…&lt;/p&gt;
&lt;p&gt;So as long as it only concerns everyday encounters and not interpersonal relations, I am all for a little bit more superficiality!&lt;/p&gt;
&lt;p&gt;Have a nice day!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>See Europe in two weeks…</title>
      <link>https://dirkhovy.com/post/2009_01_04_see_europe_in_two_weeks/</link>
      <pubDate>Sun, 04 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2009_01_04_see_europe_in_two_weeks/</guid>
      <description>&lt;p&gt;Europeans often make fun of the American tendency to see Europe in five days ( “If it is Tuesday, this must be Paris”). Of course you cannot fully appreciate the complexity and nuances of a country in such short a time, but in the meantime I know why one would still want to do it. And Europeans usually have more than two weeks worth of holidays per year…&lt;/p&gt;
&lt;p&gt;If you have only a restricted time, you try to cram as much into it as possible―if you like to travel, countries, if you like people, meetings.&lt;/p&gt;
&lt;p&gt;Over the last two weeks I have had a meeting marathon which sometime got me quite dizzy ( “If I meet X today, it must be Tuesday”), and still I did not manage to see everyone I wanted. A fortnight is far too short a time for all the wonderful people you meet over almost three decades…&lt;/p&gt;
&lt;p&gt;This always leaves the feeling, though, to just scrape at the surface and not do everyone justice. You can not fully appreciate the complexity and nuances of a person in such short a time, and still you try. Because even a brief meeting is better than none. And you take home so much more than Facebook or Skype could ever tell you.&lt;/p&gt;
&lt;p&gt;To those I met: Thanks for the time with you, it was great to see you again! And to those I did not manage to meet: Please don’t hold it against me, it was not on purpose. Just on a tight schedule…&lt;/p&gt;
&lt;p&gt;In any case I would be stoked to see you in LA at one point. Maybe if you are on a US trip? In that case, why not make it a Tuesday?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speak in tongues</title>
      <link>https://dirkhovy.com/post/2008_11_27_speak_in_tongues/</link>
      <pubDate>Thu, 27 Nov 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_11_27_speak_in_tongues/</guid>
      <description>&lt;p&gt;Language is one of the things you always take with you, no matter where, when, or what your baggage restrictions are. Our phonological system is hard to fool (or, as Christoph says  “phonology always works”). If you don’t believe it, try for one hour to exchange all Fs and Ks. Should you succeed, you are a genius. If not, you’ll have a lot of fun.&lt;/p&gt;
&lt;p&gt;This leaves us with the realisation, though, that we will always be spotted a s foreigners: German final devoicing and the whole trouble with  “th” and  “wh” are dead giveaways, and my  “vowels are not American”, as a friend pointed out.&lt;/p&gt;
&lt;p&gt;Language is not only grammar and vocabulary, but also pronunciation subtleties: American  “sh “s have less friction than German ones, less rounding, and the  “a “s in  “aber” and  “garden” are absolutely not the same. Over the past few months, I have been identified as South African, British, or, well: German, but nobody ever seriously considered me to be American.&lt;/p&gt;
&lt;p&gt;Even as a linguist you cannot beat your own system: The brain happily abstracts and throws everything into neat categories. Don’t bother it with details…&lt;/p&gt;
&lt;p&gt;I have tried to pick up a few Chinese phrases, but my Chinese friends have either smiled politely or sadly shaken their heads. Even when I thought that I had repeated everything I heard, I hadn’t, since Chinese not only uses sounds, but also tones, and if you are not trained, you frankly don’t hear them…&lt;/p&gt;
&lt;p&gt;It is only slightly consoling that on the other side, foreigners never get a German  “ch” right.&lt;/p&gt;
&lt;p&gt;It gets even worse when we get to meaning: Subconsciously, one a builds up a fine grained taxonomy of meaning nuances: Langauge is like a well worn rapier, which can pinpoint a meaning and win an argument.&lt;/p&gt;
&lt;p&gt;Only when you start argueing in another language you realize you are suddenly handling a club. Sure, you can hit at the general meaning area, and you can win an argument, provided you hit first and hard. Yet it has no elegance or style, and too often, one is left searching for the right words to express a thought.&lt;/p&gt;
&lt;p&gt;There are words, though, that I would like to have in both languages:  “random” is such a word. I know that it can be translated as  “zufällig”, but that does not cut it in a sentence like  “That comment was so random!” And why does English not have an equivalent for  “doch “:  “yes, it &lt;em&gt;is&lt;/em&gt;” is clumsy, and does not guarantee that satisfaction to prove someone wrong with just one word (I was also told that  “jein” should be introduced, being an indecisive mix of  “yes” and  “no”).&lt;/p&gt;
&lt;p&gt;One of the biggest obstacles in learning Engish is the fact that over the years, it has acquired Scandinavian, Germanic and Romance influences and mixed it all up. There is irregular inflection, yet not consistently: goose-geese and foot-feet, yet not moose-meese or wood-weed. I try to advocate the innovative use of  “one shoop, two sheep”, yet people seem reluctant to take it on.&lt;/p&gt;
&lt;p&gt;The only way out of this seems to me founding my own language which incorporates all these wonderful concepts. As a result, nobody will understand what I am saying any more, but I guess that is the price you have to pay if you want to express yourself clearly…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shake it, baby</title>
      <link>https://dirkhovy.com/post/2008_11_14_shake_it_baby/</link>
      <pubDate>Fri, 14 Nov 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_11_14_shake_it_baby/</guid>
      <description>&lt;p&gt;Yesterday, we had an earthquake drill. At 10:00am, we were supposed to  “drop, cover, and hold on” (does  “duck and cover” sound familiar). Since I am all for this kind of prevention, I participated. Apparently, I was about the only person who did… After five rather dull minutes (no people playing wounded, no paramedic coming to check on me), a rather puzzled colleague inquired what I was doing and whether I was ok. Apart from the fact that the space under my desk is rather dark and claustrophobic, I was, and I would have been in case of a real earthquake. That is, unless the building collapsed. In that case, you should sit next to your desk and hope that the falling debris forms a cave around it.&lt;/p&gt;
&lt;p&gt;Given that I work on the fourth floor of a 12 storey building, I am not convinced that would help much…&lt;/p&gt;
&lt;p&gt;It might be better to be in an elevator, since those swing freely in a concrete shaft inside the building (outer walls are the most dangerous ones). Unless the cable snaps or a fire breaks out.&lt;/p&gt;
&lt;p&gt;I guess you just have to hope the architects did their best and you manage to stay away from the windows and outer walls when an earthquake hits. And if  “drop, cover and hold on” helps―so be it.&lt;/p&gt;
&lt;p&gt;I’d still rather not try it…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>News from Behind the Mirror’s Glass</title>
      <link>https://dirkhovy.com/post/2008_11_08_news_from_behind_the_mirrors_glass/</link>
      <pubDate>Sat, 08 Nov 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_11_08_news_from_behind_the_mirrors_glass/</guid>
      <description>&lt;p&gt;I am happy to announce that the little spider living behind my left rearview mirror―despite a major car wash―continues to weave her net everyday.&lt;/p&gt;
&lt;p&gt;In case you were worried…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It’s over</title>
      <link>https://dirkhovy.com/post/2008_11_04_its_over/</link>
      <pubDate>Tue, 04 Nov 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_11_04_its_over/</guid>
      <description>&lt;p&gt;As a friend said:  “I only believe when Fox announces Obama as president”. Sen. McCain just acknowledged Obama’s victory in a fair and moving speech―and Fox confirmed it…&lt;/p&gt;
&lt;p&gt;That’s it: America has a new president!&lt;/p&gt;
&lt;p&gt;Probably not all will be well now, but hopefully many things a lot better. Thanks for voting, America. I am a happy alien…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A classic</title>
      <link>https://dirkhovy.com/post/2008_11_01_a_classic/</link>
      <pubDate>Sat, 01 Nov 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_11_01_a_classic/</guid>
      <description>&lt;p&gt;Since the first mentioning of my car here, it has not been washed. As LA is a very dusty city, however, so it was about time to change that. I thus welcomed the fact that a sorority of the local college held a car wash in my street to raise money for a school of blind children. Getting my car washed for a small fee for a good cause―that seemed ike a pretty good deal to me.&lt;/p&gt;
&lt;p&gt;Probably a bit too good to be true. For about two hours, I was the happy owner of a shiny car. Then, the long awaited rain set in and washed all the dirt out of the air―and onto my car.&lt;/p&gt;
&lt;p&gt;In retalliation, I went and had my hair cut―that was also long overdue, yet it won’t be ruined by the next downpour…&lt;/p&gt;
&lt;p&gt;That’ll show the wheather…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Persistency</title>
      <link>https://dirkhovy.com/post/2008_10_25_persistency/</link>
      <pubDate>Sat, 25 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_10_25_persistency/</guid>
      <description>&lt;p&gt;Behind the left rear mirror of my car lives a little spider, which comes out every night and weaves her net between the door and the mirror. And every morning when I get into my car, I remove the net.&lt;/p&gt;
&lt;p&gt;It is nothing personal from my side, and in secret, I even admire her persistency, but it has become some sort of ritual.&lt;/p&gt;
&lt;p&gt;Maybe there is something I can learn from it: Get out every morning and weave your web…&lt;/p&gt;
&lt;p&gt;Or probably I just have to start to wash my car properly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>At least…</title>
      <link>https://dirkhovy.com/post/2008_10_17_at_least/</link>
      <pubDate>Fri, 17 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_10_17_at_least/</guid>
      <description>&lt;p&gt;The last few weeks have been pretty intense (workload and stress level-wise). The next weeks will see more of that, but at least I am better prepared now.&lt;/p&gt;
&lt;p&gt;Also, I found out that I can see the Hollywood sign from my office window.&lt;/p&gt;
&lt;p&gt;Well, that’s something…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The town that wasn’t there</title>
      <link>https://dirkhovy.com/post/2008_09_18_the_town_that_wasnt_there/</link>
      <pubDate>Thu, 18 Sep 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_09_18_the_town_that_wasnt_there/</guid>
      <description>&lt;p&gt;Ok, so I am in LA: Everything is loud, big, and exciting. I have finally a room and all the accompanying bits and pieces, and life goes its way. Besides all that organizational matters I did at least not have to worry about getting to know the city. Which is an advantage. Not a very individual one, granted. We all know it. We all have been here. At least if we had a television…&lt;/p&gt;
&lt;p&gt;The A Team, Baywatch, Beverly Hills 90210, and countless other series portray LA, even without stating that explicitly. Hollywood is not only a place, but a trademark and―to some―a way of life.&lt;/p&gt;
&lt;p&gt;Musicians from Cypress Hill over Presidents of the USA to Sheryl Crow or System of a Down sing about the city, telling tales about the hard life in South Central or the bars in Downtown, and A Tribe Called Quest in an older song bemoaned the loss of their wallet in El Segundo.&lt;/p&gt;
&lt;p&gt;Yes, LA is exactly as we always imagined it to be. We have all been here, we know how it looks. Other than Bielefeld it exists, it has a place in our heads. That also means, however, that everyone has an exact picture about how I live. They differ somewhat, but an astounding number of them involve pools, stars, yet also thugs and shopping carts.&lt;/p&gt;
&lt;p&gt;My reality is somewhat different: Sure, everything is as seen in the movies, all the places do exist. There are thugs, there are stars, and there are ghettos and glamour side by side. Reality, however, is less glamorous: I go to university in South Central, El Segundo is just a 15min bike ride away, Beverly Hills and I are separated by just a few numbers in our ZIP code, and Hollywood is a dingy quarter full of tourists, hookers, and souvenir shops.  I live in two cities…&lt;/p&gt;
&lt;p&gt;Up close, the city loses much of its screen character and becomes something else: A tangle of streets and houses, of beach and highways. It is many small cities in one, all full of interesting, busy, and mostly nice people from all over the world. It is a myth that keeps re-inventing itself, an unsentimental giant with a disposition for drama. A contradiction in itself.&lt;/p&gt;
&lt;p&gt;No, this LA is certainly not a pretty city, yet an exciting and―mostly―a friendly one.&lt;/p&gt;
&lt;p&gt;And currently my home.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Art of Self-Contradiction</title>
      <link>https://dirkhovy.com/post/2008_09_08_the_art_of_selfcontradiction/</link>
      <pubDate>Mon, 08 Sep 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_09_08_the_art_of_selfcontradiction/</guid>
      <description>&lt;p&gt;Currently reading a book which states that you should not repeat yourself.&lt;/p&gt;
&lt;p&gt;To make sure I got that they say it several times…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My house, my car, my…</title>
      <link>https://dirkhovy.com/post/2008_08_24_my_house_my_car_my/</link>
      <pubDate>Sun, 24 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_08_24_my_house_my_car_my/</guid>
      <description>&lt;p&gt;“When I arrived, all I had were two bags full of stuff, and now I am the owner of…”&lt;/p&gt;
&lt;p&gt;We all know the stories starting out like that. They are part of the topos America, just as the rich uncle and the dish washer turned millionaire.&lt;/p&gt;
&lt;p&gt;In my case, however, the story is not that compelling yet―I have neither founded a global trading empire nor bought a villa with celebrity neighbours in Beverly Hills. In fact, I still do not even own a room yet.&lt;/p&gt;
&lt;p&gt;Yet my possession has grown considerately since the notorious two bags.&lt;/p&gt;
&lt;p&gt;Since yesterday, I own a new iPhone and―just borrowed, though―an old car. What the car lacks in glamour is made up for by the phone: It is chic, shiny, easy to use out of the box―and devours battery like there is no tomorrow.&lt;/p&gt;
&lt;p&gt;The car, in contrast, is rather energy efficient, an old Honda that does not need much.&lt;/p&gt;
&lt;p&gt;I hate to admit it, yet with both I am pretty much in line with the current L.A. trend: While a year ago the streets were full of cars that consumed more fuel on starting than a weekend trip through Europe did, fuel efficiency is now the new cool.  And with the radical consequence that characterizes Americans and especially Californians, they have changed the outlook of their streets.&lt;/p&gt;
&lt;p&gt;I do not dare to tell people that they still pay half the price they would in Germany―if they set their minds on energy conscious living, who am I to stop them? My car will play its part, and my conscience can be at peace. Yet is my phone a green one?&lt;/p&gt;
&lt;p&gt;So, having entered the world of the propertied with all the entailing moral consequences, all I lack now is a room to put all that stuff into. And a moral guideline to buy stuff…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>If a bit under the weather…</title>
      <link>https://dirkhovy.com/post/2008_08_22_if_a_bit_under_the_weather/</link>
      <pubDate>Fri, 22 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_08_22_if_a_bit_under_the_weather/</guid>
      <description>&lt;p&gt;One of the questions posed most frequently to me over the last few days was  “How’s the weather?” Quite understandable if you are from a country that―due to changing metereological conditions―has something like weather.&lt;/p&gt;
&lt;p&gt;Here in L.A., however, the weatherforecast is about as interesting as election results from a totalitarian regime: You know beforehand what it’s going to be. In this case: 80°F plusminus 5, a little cooler in the mornings and evenings.&lt;/p&gt;
&lt;p&gt;That may sound great, yet after a few days you get used to it. You also find out quickly that the clouds you see in the morning will have disappeared until noon.&lt;/p&gt;
&lt;p&gt;As exciting as the city is otherwise, the weather is rather bland.&lt;/p&gt;
&lt;p&gt;At least this has the advantage that you can spend more time worrying about other things than umbrellas and warm socks. For example forms. But that is another story. One you just have to weather…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Babel</title>
      <link>https://dirkhovy.com/post/2008_08_16_babel/</link>
      <pubDate>Sat, 16 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_08_16_babel/</guid>
      <description>&lt;p&gt;Well, I knew that das passieren würde, but when it strikes, you feel immer a little lost. After zwei oder three Tagen, your Gehirn does not know genau, which language to take. Es ist no longer German, aber it ist not yet Englisch. You concentrate on eine Sprache, but you keep switching back und vor.&lt;/p&gt;
&lt;p&gt;It does nicht mal help if your host understands beide Sprachen, because you will not know, welchen Ausdruck man what zuordnen muss. I feel wie ein Aphasiker, der things zwar beschreiben can, yet is unable to recollect the Namen. It is schlimm genug, einen expression in the foreign language nicht zu wissen, yet I forgot them in either der beiden Sprachen!&lt;/p&gt;
&lt;p&gt;At least this will sort sich out after a few days, aber until then I will re-enact the babylonian Sprachgewirr in meinem head…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arrived</title>
      <link>https://dirkhovy.com/post/2008_08_14_arrived/</link>
      <pubDate>Thu, 14 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_08_14_arrived/</guid>
      <description>&lt;p&gt;After a lot of toil and trouble, much waiting and even more forms, I have finally arrived in Los Angeles. Let’s start…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Packed</title>
      <link>https://dirkhovy.com/post/2008_08_14_packed/</link>
      <pubDate>Thu, 14 Aug 2008 00:00:00 +0000</pubDate>
      <guid>https://dirkhovy.com/post/2008_08_14_packed/</guid>
      <description>&lt;p&gt;Two bags. That’s it. Two bags, max. 32kg each, more does the airline not allow. Two bags to take everything with you. Not only the stuff you need, but also everthing that separates travelling from living.&lt;/p&gt;
&lt;p&gt;It’s strange to confine your former life to a certain weight and some baggage dimensions, but what can you do&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
